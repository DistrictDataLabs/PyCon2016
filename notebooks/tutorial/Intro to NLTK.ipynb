{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as data scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practioners have benefitted from machine learning techniques to unlock meaning from large corpora, and in this tutorial we’ll explore how to do that using Python, the Natural Language Toolkit (NLTK) and Gensim. \n",
    "\n",
    "NLTK is an excellent library for machine-learning based NLP, written in Python by experts from both academia and industry. Python allows you to create rich data applications rapidly, iterating on hypotheses. The combination of Python + NLTK means that you can easily add language-aware data products to your larger analytical workflows and applications. \n",
    "\n",
    "## Quick Overview of NLTK\n",
    "NLTK was written by two eminent computational linguists, Steven Bird (Senior Research Associate of the LDC and professor at the University of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). The NTLK library provides a combination of natural language corpora, lexical resources, and example grammars with language processing algorithms, methodologies and demonstrations for a very Pythonic \"batteries included\" view of natural language processing.   \n",
    "\n",
    "As such, NLTK is perfect for research-driven (hypothesis-driven) workflows for agile data science. \n",
    "\n",
    "### Installing NLTK\n",
    "\n",
    "This notebook has a few dependencies, most of which can be installed via the python package manger - `pip`. \n",
    "\n",
    "1. Python 2.7+ or 3.5+ (Anaconda is ok)\n",
    "2. NLTK\n",
    "3. The NLTK corpora \n",
    "4. The BeautifulSoup library\n",
    "5. The gensim libary\n",
    "\n",
    "Once you have Python and pip installed you can install NLTK from the terminal as follows:\n",
    "\n",
    "```bash\n",
    "~$ pip install nltk\n",
    "~$ pip install matplotlib\n",
    "~$ pip install beautifulsoup4\n",
    "~$ pip install gensim\n",
    "```\n",
    "\n",
    "Note that these will also install Numpy and Scipy if they aren't already installed. \n",
    "\n",
    "### What NLTK Includes\n",
    "\n",
    "- [tokenization](#tokenize), [stemming](#stemming), and [tagging](#pos)\n",
    "- [chunking](#chunk) and [parsing](#parse)\n",
    "- language modeling\n",
    "- [classification](#classify) and clustering\n",
    "- logical semantics\n",
    "\n",
    "NLTK is a useful pedagogical resource for learning NLP with Python and serves as a starting place for producing production grade code that requires natural language analysis. It is also important to understand what NLTK is _not_.\n",
    "\n",
    "### What NLTK is _Not_\n",
    "\n",
    "- Production ready out of the box\n",
    "- Lightweight\n",
    "- Generally applicable\n",
    "- Magic\n",
    "\n",
    "NLTK provides a variety of tools that can be used to explore the linguistic domain but is not a lightweight dependency that can be easily included in other workflows, especially those that require unit and integration testing or other build processes. This stems from the fact that NLTK includes a lot of added code but also a rich and complete library of corpora that power the built-in algorithms. \n",
    "\n",
    "### The Good Parts of NLTK\n",
    "\n",
    "- Preprocessing\n",
    "    - [segmentation](#segment)\n",
    "    - [tokenization](#tokenize)\n",
    "    - [Part-of-Speech (PoS) tagging](#pos)\n",
    "- Word level processing\n",
    "    - [WordNet](#wordnet)\n",
    "    - [Lemmatization](#lemmatize)\n",
    "    - [Stemming](#stemming)\n",
    "    - [NGrams](#ngram)\n",
    "- Utilities\n",
    "    - Tree\n",
    "    - [FreqDist](#freqdist)\n",
    "    - ConditionalFreqDist\n",
    "    - Streaming CorpusReaders\n",
    "- [Classification](#classify)\n",
    "    - Maximum Entropy\n",
    "    - Naive Bayes\n",
    "    - Decision Tree\n",
    "- [Chunking](#chunk)\n",
    "- [Named Entity Recognition](#nerc)\n",
    "- [Parsers Galore!](#parse)\n",
    "\n",
    "### The Bad parts of NLTK\n",
    "\n",
    "- Syntactic Parsing\n",
    "\n",
    "    - No included grammar (not a black box)\n",
    "    - No Feature/Dependency Parsing\n",
    "    - No included feature grammar\n",
    "\n",
    "- The sem package\n",
    "    \n",
    "    - Toy only (lambda-calculus & first order logic)\n",
    "\n",
    "- Lots of extra stuff (heavyweight dependency)\n",
    "\n",
    "    - papers, chat programs, alignments, etc.\n",
    "\n",
    "Knowing the good and the bad parts will help you explore NLTK further - looking into the source code to extract the material you need, then moving that code to production. We will explore NLTK in more detail in the rest of this notebook. \n",
    "\n",
    "## Obtaining and Exploring the NLTK Corpora\n",
    "NLTK ships with a variety of corpora, let's use a few of them to do some work. To download the NLTK corpora, open a Python interpreter:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "This will open up a window with which you can download the various corpora and models to a specified location. For now, go ahead and download it all as we will be exploring as much of NLTK as we can. Also take note of the `download_directory` - you're going to want to know where that is so you can get a detailed look at the corpora that's included. I usually export an environment variable to track this. You can do this from your terminal:\n",
    "\n",
    "    ~$ export NLTK_DATA=/path/to/nltk_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StanfordTokenizer',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'accuracy',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'alignment_error_rate',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'class_types',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_function',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'python_2_unicode_compatible',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'six',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'string_types',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'text',\n",
       " 'text_type',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to explore what is in this directory\n",
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for Working with Sample NLTK Corpora\n",
    "\n",
    "To explore much of the built-in corpus, use the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LazyModule__lazymodule_globals\n",
      "_LazyModule__lazymodule_import\n",
      "_LazyModule__lazymodule_init\n",
      "_LazyModule__lazymodule_loaded\n",
      "_LazyModule__lazymodule_locals\n",
      "_LazyModule__lazymodule_name\n",
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattr__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__name__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n"
     ]
    }
   ],
   "source": [
    "# Lists the various corpora and CorpusReader classes in the nltk.corpus module\n",
    "for name in dir(nltk.corpus):\n",
    "    print(name)\n",
    "#     if name.islower() and not name.startswith('_'): print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fileids()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# You can explore the titles with:\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', 'macbeth.xml', 'merchant.xml', 'othello.xml', 'r_and_j.xml']\n"
     ]
    }
   ],
   "source": [
    "# For a specific corpus, list the fileids that are available:\n",
    "print(nltk.corpus.shakespeare.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `text.Text()`\n",
    "\n",
    "The `nltk.text.Text` class is a wrapper around a sequence of simple (string) tokens - intended only for _the initial exploration of text_ usually via the Python REPL. It has the following methods:\n",
    "\n",
    "- common_contexts\n",
    "- concordance\n",
    "- collocations\n",
    "- count\n",
    "- plot\n",
    "- findall\n",
    "- index\n",
    "\n",
    "You shouldn't use this class in production level systems, but it is useful to explore (small) snippets of text in a meaningful fashion. \n",
    "\n",
    "For example, you can get access to the text from Hamlet as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamlet = nltk.text.Text(nltk.corpus.gutenberg.words('shakespeare-hamlet.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `concordance()`\n",
    "The concordance function performs a search for the given token and then also provides the surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 172 matches:\n",
      "elfe Bar . Long liue the King Fran . Barnardo ? Bar . \n",
      "e same figure , like the King that ' s dead Mar . Thou\n",
      ". Lookes it not like the King ? Marke it Horatio Hora \n",
      "Mar . Is it not like the King ? Hor . As thou art to t\n",
      "isper goes so : Our last King , Whose Image euen but n\n",
      "mpetent Was gaged by our King : which had return ' d T\n",
      "Secunda . Enter Claudius King of Denmarke , Gertrude t\n",
      "elia , Lords Attendant . King . Though yet of Hamlet o\n",
      "er To businesse with the King , more then the scope Of\n",
      " , will we shew our duty King . We doubt it nothing , \n"
     ]
    }
   ],
   "source": [
    "hamlet.concordance(\"king\", 55, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `similar()`\n",
    "\n",
    "Given some context surrounding a word, we can discover similar words, e.g. words that that occur frequently in the same context and with a similar distribution: Distributional similarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greefe it clouds that thewes faculty paris heauen reputation time t\n",
      "wrath eare funerall loue forme together\n",
      "None\n",
      "\n",
      "mother family affection side brother time life head eyes sister\n",
      "judgment house arrival hair ease acquaintance own regard daughters\n",
      "care\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hamlet.similar(\"marriage\"))\n",
    "austen = nltk.text.Text(nltk.corpus.gutenberg.words(\"austen-sense.txt\"))\n",
    "print()\n",
    "print(austen.similar(\"marriage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this takes a bit of time to build the index in memory, one of the reasons it's not suggested to use this class in production code. \n",
    "\n",
    "#### `common_contexts()`\n",
    "\n",
    "Now that we can do searching and similarity, we find the common contexts of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_that my_and\n"
     ]
    }
   ],
   "source": [
    "hamlet.common_contexts([\"king\", \"father\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your turn, go ahead and explore similar words and contexts - what does the common context mean?_\n",
    "\n",
    "#### `dispersion_plot()`\n",
    "\n",
    "NLTK also uses matplotlib and pylab to display graphs and charts that can show dispersions and frequency. This is especially interesting for the corpus of innagural addresses given by U.S. presidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The plot function requires matplotlib to be installed.See http://matplotlib.org/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/benjamin/Repos/ddl/pycon2016/notebooks/tutorial/venv/lib/python3.5/site-packages/nltk/draw/dispersion.py\u001b[0m in \u001b[0;36mdispersion_plot\u001b[0;34m(text, words, ignore_case, title)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-43980b83070d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minaugural\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispersion_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"citizens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"democracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"freedom\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"duty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"America\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/benjamin/Repos/ddl/pycon2016/notebooks/tutorial/venv/lib/python3.5/site-packages/nltk/text.py\u001b[0m in \u001b[0;36mdispersion_plot\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \"\"\"\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispersion_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mdispersion_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benjamin/Repos/ddl/pycon2016/notebooks/tutorial/venv/lib/python3.5/site-packages/nltk/draw/dispersion.py\u001b[0m in \u001b[0;36mdispersion_plot\u001b[0;34m(text, words, ignore_case, title)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         raise ValueError('The plot function requires matplotlib to be installed.'\n\u001b[0m\u001b[1;32m     28\u001b[0m                      'See http://matplotlib.org/')\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The plot function requires matplotlib to be installed.See http://matplotlib.org/"
     ]
    }
   ],
   "source": [
    "inaugural = nltk.text.Text(nltk.corpus.inaugural.words())\n",
    "inaugural.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duty\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.fileids())\n",
    "nltk.corpus.stopwords.words('english')\n",
    "import string\n",
    "# print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These corpora export several vital methods:\n",
    "\n",
    "- paras (iterate through each paragraph)\n",
    "- sents (iterate through each sentence)\n",
    "- words (iterate through each word)\n",
    "- raw   (get access to the raw text)\n",
    "\n",
    "#### `paras()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']], [['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']], ...]\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.brown\n",
    "print(corpus.paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sents()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `words()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `raw()`\n",
    "\n",
    "Be careful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' tha\n"
     ]
    }
   ],
   "source": [
    "print(corpus.raw()[:200]) # Be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your turn! Explore some of the text in the available corpora_\n",
    "\n",
    "<a id='freqdist'></a>\n",
    "## Frequency Analyses \n",
    "\n",
    "In statistical machine learning approaches to NLP, the very first thing we need to do is count things - especially the unigrams that appear in the text and their relationships to each other. NLTK provides two excellent classes to enable these frequency analyses:\n",
    "\n",
    "- `FreqDist`\n",
    "- `ConditionalFreqDist` \n",
    "\n",
    "And these two classes serve as the foundation for most of the probability and statistical analyses that we will conduct.\n",
    "\n",
    "### Zipf's Law\n",
    "\n",
    "![Zipf's distribution](images/Zipf_distribution_PMF.png)\n",
    "\n",
    "Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. Read more on [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "\n",
    "First we will compute the following:\n",
    "\n",
    "- The count of words\n",
    "- The vocabulary (unique words)\n",
    "- The lexical diversity (the ratio of word count to vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 41600 types and 1720901 tokens for a lexical diversity of 41.368\n"
     ]
    }
   ],
   "source": [
    "reuters = nltk.corpus.reuters # Corpus of news articles\n",
    "counts  = nltk.FreqDist(reuters.words())\n",
    "vocab   = len(counts.keys())\n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print(\"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.B()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `most_common()`\n",
    "\n",
    "The _n_ most common tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037), ('vs', 14120), ('-', 13705), ('for', 12785), ('dlrs', 11730), (\"'\", 11272), ('The', 10968), ('000', 10277), ('1', 9977), ('s', 9298), ('pct', 9093), ('it', 8842), (';', 8762), ('&', 8698), ('lt', 8694), ('on', 8556), ('from', 7986), ('cts', 7953), ('is', 7580), ('>', 7449), ('that', 7377), ('its', 7265), ('by', 6872), ('\"', 6816), ('at', 6537), ('2', 6528), ('U', 6388), ('S', 6382), ('year', 6310), ('be', 6288), ('with', 5945)]\n"
     ]
    }
   ],
   "source": [
    "print(counts.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `counts.max()`\n",
    "\n",
    "The most frequent token in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(counts.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `counts.hapaxes()`\n",
    "\n",
    "A list of all _hapax legomena_ (words that only appear one time in the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liens', 'BUREAU', 'wh', 'extraneous', 'Guasare', 'canals', 'SPIKA', 'Farmland', 'hospitalized', 'Barings']\n"
     ]
    }
   ],
   "source": [
    "print(counts.hapaxes()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `counts.freq()`\n",
    "\n",
    "The percentage of the corpus for the given token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.810909517746808e-05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.freq('stipulate') * 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `counts.plot()`\n",
    "\n",
    "Plot the frequencies of the _n_ most commonly occuring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The plot function requires matplotlib to be installed.See http://matplotlib.org/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/benjamin/Repos/ddl/pycon2016/notebooks/tutorial/venv/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-213f2426ac57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumulative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/benjamin/Repos/ddl/pycon2016/notebooks/tutorial/venv/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             raise ValueError('The plot function requires matplotlib to be installed.'\n\u001b[0m\u001b[1;32m    232\u001b[0m                          'See http://matplotlib.org/')\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The plot function requires matplotlib to be installed.See http://matplotlib.org/"
     ]
    }
   ],
   "source": [
    "counts.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# By setting cumulative to True, we can visualize the cumulative counts of the _n_ most common words.\n",
    "counts.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ConditionalFreqDist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lore: 14503 types with 110299 tokens and lexical diversity of 7.605\n",
      "religion: 6373 types with 39399 tokens and lexical diversity of 6.182\n",
      "humor: 5017 types with 21695 tokens and lexical diversity of 4.324\n",
      "hobbies: 11935 types with 82345 tokens and lexical diversity of 6.899\n",
      "science_fiction: 3233 types with 14470 tokens and lexical diversity of 4.476\n",
      "editorial: 9890 types with 61604 tokens and lexical diversity of 6.229\n",
      "romance: 8452 types with 70022 tokens and lexical diversity of 8.285\n",
      "belles_lettres: 18421 types with 173096 tokens and lexical diversity of 9.397\n",
      "learned: 16859 types with 181888 tokens and lexical diversity of 10.789\n",
      "fiction: 9302 types with 68488 tokens and lexical diversity of 7.363\n",
      "mystery: 6982 types with 57169 tokens and lexical diversity of 8.188\n",
      "reviews: 8626 types with 40704 tokens and lexical diversity of 4.719\n",
      "government: 8181 types with 70117 tokens and lexical diversity of 8.571\n",
      "adventure: 8874 types with 69342 tokens and lexical diversity of 7.814\n",
      "news: 14394 types with 100554 tokens and lexical diversity of 6.986\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain \n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "categories = brown.categories()\n",
    "\n",
    "counts = nltk.ConditionalFreqDist(chain(*[[(cat, word) for word in brown.words(categories=cat)] for cat in categories]))\n",
    "\n",
    "for category, dist in counts.items():\n",
    "    vocab  = len(dist.keys())\n",
    "    tokens = sum(dist.values())\n",
    "    lexdiv = float(tokens) / float(vocab)\n",
    "    print(\"%s: %i types with %i tokens and lexical diversity of %0.3f\" % (category, vocab, tokens, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your turn: compute the conditional frequency distribution of bigrams in a corpus_\n",
    "\n",
    "Hint:\n",
    "\n",
    "<a id='ngram'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'bear', 'walked', 'in', 'the')\n",
      "('bear', 'walked', 'in', 'the', 'woods')\n",
      "('walked', 'in', 'the', 'woods', 'at')\n",
      "('in', 'the', 'woods', 'at', 'midnight')\n"
     ]
    }
   ],
   "source": [
    "for ngram in nltk.ngrams([\"The\", \"bear\", \"walked\", \"in\", \"the\", \"woods\", \"at\", \"midnight\"], 5):\n",
    "    print(ngram)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "\n",
    "NLTK is great at the preprocessing of raw text - it provides the following tools for dividing text into it's constituent parts:\n",
    "<a id='tokenize'></a>\n",
    "<a id='segment'></a>\n",
    "- `sent_tokenize`: a Punkt sentence tokenizer:\n",
    "\n",
    "    This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.  It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "    \n",
    "    However, Punkt is designed to learn parameters (a list of abbreviations, etc.) unsupervised from a corpus similar to the target domain. The pre-packaged models may therefore be unsuitable: use PunktSentenceTokenizer(text) to learn parameters from the given text.\n",
    "    \n",
    "    \n",
    "- `word_tokenize`: a Treebank tokenizer \n",
    "\n",
    "    The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This is the method that is invoked by ``word_tokenize()``.  It assumes that the text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
    "    \n",
    "<a id='pos'></a>\n",
    "- `pos_tag`: a maximum entropy tagger trained on the Penn Treebank\n",
    "\n",
    "    There are several other taggers including (notably) the BrillTagger as well as the BrillTrainer to train your own tagger or tagset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from readability.readability import Document\n",
    "\n",
    "# Tags to extract as paragraphs from the HTML text\n",
    "TAGS = [\n",
    "    'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li'\n",
    "]\n",
    "\n",
    "def read_html(path):\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        # Transform the document into a readability paper summary\n",
    "        html = Document(f.read()).summary()\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = bs4.BeautifulSoup(html)\n",
    "\n",
    "        # Extract the paragraph delimiting elements\n",
    "        for tag in soup.find_all(TAGS):\n",
    "\n",
    "            # Get the HTML node text\n",
    "            yield tag.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " It’s lowbrow. It’s messy. It could never be accused of being healthful. But we’d never let those formalities get between us and an order of crispy, crackly, delicious fried chicken. Whether it comes in a bucket or on a bun, or you eat it with your fingers or chopsticks, there’s a surprising variety to the Washington area’s fried chicken offerings. Here are some of the most irresistible. \n",
      "\n",
      "\n",
      "\n",
      "‘Rotissi-fried’ chicken at the Partisan\n",
      "\n",
      "Forget the cronut. Our newest favorite food chimera is the “rotissi-fried” chicken at the Partisan. Credit goes to chef Nate Anda, who dreamed up the dish: After a 12-hour brine, the chicken is rotisseried for two hours and then fried for two and a half minutes. Why both? “Everything is better once it’s fried in beef fat,” Anda said. We have to agree. Whether white or dark, the meat is succulent throughout. The batter-free frying leaves the simply seasoned skin rendered perfectly crisp, golden and translucent — cracklings, essentially. The sound of it shattering under the knife was music to our ears. And as if the lily needed further gilding, the chicken comes with a generous pour of honey hot sauce. The sauce is hard to resist, but try to reserve a few bites of unadorned chicken so you can fully appreciate this happy marriage of classic preparations.\n",
      "\n",
      " The Partisan, 709 D St. NW. 202-524-5322. www.thepartisandc.com.  \n",
      "\n",
      "— Becky Krystal\n",
      "\n",
      "Traditional fried chicken at Family Meal\n",
      "\n",
      "When Bryan Voltaggio started planning the menu for Family Meal, his modern, upscale spin on a diner, his thoughts turned to home and the carryout meal he most enjoyed as a kid: fried chicken. “It was one of our favorite things,” he says. “It just seems like a family dinner.” And if he was creating a restaurant called Family Meal, fried chicken “had to be an important part of it.” But Voltaggio wanted to do it right, and set about testing cooking methods, brines, breadings and fryers. That long process paid off with a home run of a fried chicken dish that’s become the most popular item on Family Meal’s menu. The whole chickens spend 12 hours in a brine of pickle juice and roasted poultry stock before getting dredged, rested and dredged again in a mixture of flour, cornmeal and corn starch. After a dip in the top-of-the-line pressure fryer, the thighs, legs and breasts emerge with a crisp, salty skin that cracks open to reveal wonderfully warm and moist flesh. You don’t even need to dunk it in the house-made hot sauce that accompanies the dish, but really, who can resist? \n",
      "\n",
      " Family Meal Ashburn, 20462 Exchange St. 703-726-9800; Frederick, 880 N. East St. 301-378-2895; Baltimore, 621 E. Pratt St. 410-601-3242. www.voltfamilymeal.com.  \n",
      "\n",
      "— John Taylor\n",
      "\n",
      "Japanese fried chicken at Izakaya Seki\n",
      "\n",
      "Although it’s commonly served in Japan at karaoke bars, convenience stores and on street carts, kara age chicken — like most of the country’s food — is held to an extremely high standard. “It’s taken it to the nth degree of obsession and detail,” says Cizuka Seki, who, with her father Hiroshi, owns Izakaya Seki on V Street NW. “Kara age” is used to describe the method for deep-frying bite-size pieces of fish and, more commonly, chicken. Though there are subtle variations on the ubiquitous dish, most recipes call for chicken thighs marinated in soy sauce, coated in flour or corn starch and deep-fried in oil. Izakaya Seki’s version sticks closely to the formula. Probably. “I’m not even quite sure what my dad puts into it, because we don’t have recipes,” Seki says, though she’s certain wheat flour is involved. The result is a thin, tender coating that’s slightly softer than tempura. The accompanying ponzu sauce lends a tartness to the nubs.\n",
      "\n",
      " Izakaya Seki, 1117 V St. NW. 202-588-5841. www.sekidc.com.  \n",
      "\n",
      "— Holley Simmons\n",
      "\n",
      "Korean fried chicken at BonChon\n",
      "\n",
      "Don’t waste your kimchi-stinking breath asking for more sauce at BonChon. The South Korean fried chicken chain, founded in 2002, is so dedicated to consistency that it doesn’t allow for any modifications. And why would you want to change anything, really? The made-to-order wings, drumsticks and strips are fried twice, resulting in a paper-thin crust that yields the same satisfying crack as shattering crème brulee with a spoon. Founder Jinduk Seh spent two years perfecting his secret sauces, which come in three flavors — soy garlic, hot and a blend of the two — and are brushed on by hand post-fry, piece by piece. True to BonChon’s commitment to uniformity, sauces are made exclusively in South Korea and distributed to all 140-plus BonChon locations, which means the wings you’re chewing on in Arlington are slathered with the same exact stuff as those in the Philippines. Joints like these are so common throughout Korea they’re called “chimeks,” which is a hybrid term that combines “chicken” with the Korean word for beer. Washington should be happy to have 10 BonChons within driving distance, plus a brand new Metro-accessible location near the Navy Yard.\n",
      "\n",
      " BonChon, 1015 Half St. SE and nine other locations in Maryland and Virginia. www.bonchon.com.  \n",
      "\n",
      "— Holley Simmons\n",
      "\n",
      "Maryland fried chicken at Crisfield Seafood and Hank’s Oyster Bar\n",
      "\n",
      "\n",
      "\n",
      "There’s not much agreement on what constitutes Maryland fried chicken. Some say it’s just a fresh Maryland chicken that’s pan-fried; others say it should be topped with white gravy, almost like a chicken-fried steak. The pan-fried chicken platter at  Crisfield Seafood is a perfect example of the former style. Half of a chicken is dredged in flour, dusted with salt and pepper, and fried in a cast-iron pan. This preparation lends a snap and crunch to the exterior, and while the meat falls off the bone, the well-seasoned breading holds on. (The chicken is available only Friday through Sunday, and frequently sells out.) The Chesapeake fried chicken at Hank’s Oyster Bar in Dupont Circle and Capitol Hill is plumper than Crisfield’s version and seasoned with Old Bay, black pepper and cayenne, but the breading is softer and less crispy. It’s brined for 24 hours and deep-fried, rather than pan-fried, and it’s served only on Sunday. \n",
      "\n",
      " Crisfield Seafood, 8012 Georgia Ave., Silver Spring. 301-589-1306. www.crisfieldseafood.com. Hank’s Oyster Bar, 1624 Q St. NW. 202-462-4265; 633 Pennsylvania Ave. SE. 202-733-1971. www.hanksoysterbar.com. \n",
      "\n",
      "— Fritz Hahn\n",
      "\n",
      "Fancy fried chicken at Central\n",
      "\n",
      "Self-consciousness may prevent you from ordering fried chicken in a white-tablecloth restaurant. It feels incongruous — gauche, almost — to dig into picnic fare at the kind of place where you should be ordering risotto or tartare or something that comes with mousse, gelee or foam. But you have to override that adult voice in the back of your head, because if you don’t, you’ll miss out on Central Michel Richard’s famed fried chicken plate ($24 at lunch, $25 for dinner), which remains as good as ever. Though it’s no longer sold by the bucket to go, Michel Richard’s KFC-inspired crispy breast and thigh come stacked atop a pool of the butteriest mashed potatoes you’ll ever taste. It’s such a dignified presentation that this most American of dishes almost could pass as (gasp!) French. Self-consciousness should, however, steer you toward using a knife and fork — not your fingers — to eat the chicken. It is, after all, that kind of a place. \n",
      "\n",
      " Central Michel Richard, 1001 Pennsylvania Ave. NW. 202-626-0015. www.centralmichelrichard.com .   \n",
      "\n",
      "\n",
      "\n",
      "— Maura Judkis\n",
      "\n",
      "\n",
      "\n",
      "Nashville hot chicken at Reserve 2216\n",
      "\n",
      "If you believe the lore, Nashville hot chicken was basically a crime of passion, created as a blistering rebuke to a no-account Romeo who couldn’t keep his hands off other women. Alas, this wolf was also a chili head who found pleasure, not pain, in this dish of revenge served hot. Decades later, chefs are latching onto this addictive form of punishment. Aaron Silverman served an ultra-refined version at Rose’s Luxury for months, and now Eric Reid, chef and co-owner of Reserve 2216 in Del Ray, has developed his own take on hot chicken, even if he’s never actually enjoyed it in Nashville. He marinates an airline cut (boneless breast with the drumette wing attached) in buttermilk and Crystal hot sauce before dredging the chicken in seasoned flour and frying it. Reid ditches the traditional white bread base in favor of collards and a side of corn bread waffles. He finishes the dish with a combination of Cajun seasonings and more Crystal hot sauce for a moist, crispy bird that bites back. But not too hard. This is Alexandria, after all. \n",
      "\n",
      " Reserve 2216, 2216 Mount Vernon Ave., Alexandria. 703-549-2889. www.drpreserve.com. \n",
      "\n",
      "— Tim Carman\n",
      "\n",
      "\n",
      "\n",
      "Fast-food fried chicken at Popeyes\n",
      "\n",
      "The sole virtue of most fast-food operations is consistency. Whether you bite into a Big Mac in Bethesda or Beijing, the sandwich should taste the same. The menu at Popeyes follows suit, but it deviates from the brand-name competition in an important respect: The signature at Popeyes could pass for home cooking (well, if your home had a vat of clean, hot oil and a person with a Southern accent tending the meal). Maybe that accounts for my occasional forays to the chicken fryer after a bum restaurant-review excursion. No matter where I eat my order, inevitably “spicy,” I know I can count on a coating that smacks of cayenne, paprika and even crushed cornflakes, and chicken that spurts with juice. The shatter is audible; the golden crumbs fly everywhere, but end up on my tongue. No one-trick pony, Popeyes has hot and tender buttermilk biscuits that bolster my favorite excuse to snack low on the food chain. Once, I got home to discover a clerk had forgotten to pack bread in my bag. I almost cried. Instead, I consoled myself with another piece of chicken. \n",
      "\n",
      " Popeyes has locations throughout the D.C. metro area. www.popeyes.com. \n",
      "\n",
      "— Tom Sietsema\n",
      "\n",
      "\n",
      "\n",
      "Fried chicken sandwich at DCity Smokehouse\n",
      "\n",
      "The fried chicken sandwich hasn’t been the same since KFC’s Double Down turned a guilty pleasure into an outright farce, using crispy white-meat fillets as both the sandwich’s primary protein and the oily handles by which you eat the monstrosity. Leave it to Rob Sonderman, pitmaster and co-owner of DCity Smokehouse, to bring dignity back to the bite. His Den-Den — named for co-creator and pitmaster-in-training Dennis Geddie — begins with boneless thighs marinated in buttermilk, hot sauce and honey. Sonderman then dredges the meat in seasoned flour before dropping the thighs into the fryer. Generously stuffed into a grilled hoagie roll with lettuce, tomato and crispy onions, the chicken is finished with two sauces, including a house-made cilantro ranch. Technically, the Den-Den ($9.25) is one of the few smokeless items on DCity’s menu (unless you count the chipotle peppers in the hot sauce). No matter. You won’t care the minute you sink your teeth into that heaping hoagie of spicy thigh meat.\n",
      "\n",
      " DCity Smokehouse, 8 Florida Ave. NW. 202-733-1919. www.dcitysmokehouse.com.  \n",
      "\n",
      "— Tim Carman\n",
      "\n",
      "\n",
      "\n",
      "Classic D.C. fried chicken at Oohh’s and Aahh’s\n",
      "\n",
      "Hearty is the appetite that can handle Oohh’s and Aahh’s chef-owner Oji Abbott’s boneless fried chicken breast without taking home leftovers. He buys local — from Hartman Meat Co. in Northeast Washington — and butterflies each 14-ounce portion ($12.95), which results in a lot of real estate for the crispy, well-seasoned coating. Abbott chalks up the consistently moist meat to proper cooking time and temperature, and to the recipe he learned from his grandmother.\n",
      "\n",
      " Oohh’s and Aahh’s, 1005 U St. NW. 202-667-7142. www.oohhsnaahhs.com. \n",
      "\n",
      "— Bonnie S. Benwick\n",
      "\n",
      "\n",
      "\n",
      "Popcorn fried chicken at Pop’s Sea Bar\n",
      "\n",
      "It’s all too easy to chomp through an order of Boardwalk Chicken at the shore-happy Pop’s Sea Bar in Adams Morgan. The bite-size pieces of dark-meat-only bird are brined for two hours, then treated to a buttermilk bath until they are coated with plain flour and flash-fried to order. A generous hand with salt and pepper just before serving means the effect of that seasoning builds as you empty the single-serving basket ($8.99). Ask for two portions of the accompanying Jersey sauce so you’ll have enough of its horseradish-y, kitchen-sink blend for every bite.\n",
      "\n",
      " Pop’s Sea Bar, 1817 Columbia Rd. NW. 202-534-3933. www.popsseabar.com. \n",
      "\n",
      "— Bonnie S. Benwick\n",
      "\n",
      "\n",
      "\n",
      "Fried chicken tenders at GBD\n",
      "\n",
      "Chicken tenders are often relegated to the children’s menu, but the chicken Tendies at GBD are a fine meal for adults and children alike. Each white-meat tender comes with a dark, crispy outer layer with a heavy dose of salt and spice. But the best part about chicken tenders is the dipping, and GBD offers nine sauces, including a take on D.C.’s own mumbo sauce, buttermilk ranch, chipotle barbecue and Frankenbutter, which combines Frank’s RedHot sauce with butter. Ask for the $5.50 Saucetown option to try all nine. \n",
      "\n",
      " GBD, 1323 Connecticut Ave. NW. 202-524-5210. www.gbdchickendoughnuts.com. \n",
      "\n",
      "— Margaret Ely\n",
      "\n",
      "\n",
      "\n",
      "Fried chicken skins at Gypsy Soul\n",
      "\n",
      "Fried chicken fans can argue the merits of white meat vs. dark meat, or whether it’s better to chow down on a drumstick or the breast. But one thing we all can agree on is that the outer layer — the breading and the skin — is the most important element of a memorable piece of fried chicken. And sometimes you just want to savor the flavor of the skin — deeply spiced, perfect crunch — without filling up on meat or having to deal with bones. And that is when you grab one of the bar stools at R.J. Cooper’s Gypsy Soul in Fairfax’s Mosaic District. Cooper’s chicken skins ($9) are twisted slivers and shards, decadently salty and crackling with paprika, cayenne pepper and garlic. The dish arrives with a house-made “roof top honey-snake oil,” but it’s best to let these beauties shine on their own. \n",
      "\n",
      " Gypsy Soul, 8296 Glass Alley, Fairfax. 703-992-0933. www.gypsysoul-va.com. \n",
      "\n",
      "— Fritz Hahn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamin/Repos/ddl/pycon2016/notebooks/tutorial/venv/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "for paragraph in read_html('fixtures/nrRB0.html'):\n",
    "    print(paragraph + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical personnel returning to New York and New Jersey from the Ebola-riddled countries in West Africa will be automatically quarantined if they had direct contact with an infected person, officials announced Friday.\n",
      "\n",
      "New York Gov.\n",
      "\n",
      "Andrew Cuomo (D) and New Jersey Gov.\n",
      "\n",
      "Chris Christie (R) announced the decision at a joint news conference Friday at 7 World Trade Center.\n",
      "\n",
      "“We have to do more,” Cuomo said.\n",
      "\n",
      "“It’s too serious of a situation to leave it to the honor system of compliance.” They said that public-health officials at John F. Kennedy and Newark Liberty international airports, where enhanced screening for Ebola is taking place, would make the determination on who would be quarantined.\n",
      "\n",
      "Anyone who had direct contact with an Ebola patient in Liberia, Sierra Leone or Guinea will be quarantined.\n",
      "\n",
      "In addition, anyone who traveled there but had no such contact would be actively monitored and possibly quarantined, authorities said.\n",
      "\n",
      "This news came a day after a doctor who had treated Ebola patients in Guinea was diagnosed in Manhattan, becoming the fourth person diagnosed with the virus in the United States and the first outside of Dallas.\n",
      "\n",
      "And the decision came not long after a health-care worker who had treated Ebola patients arrived at Newark, one of five airports where people traveling from West Africa to the United States are encountering the stricter screening rules.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = u\"Medical personnel returning to New York and New Jersey from the Ebola-riddled countries in West Africa will be automatically quarantined if they had direct contact with an infected person, officials announced Friday. New York Gov. Andrew Cuomo (D) and New Jersey Gov. Chris Christie (R) announced the decision at a joint news conference Friday at 7 World Trade Center. “We have to do more,” Cuomo said. “It’s too serious of a situation to leave it to the honor system of compliance.” They said that public-health officials at John F. Kennedy and Newark Liberty international airports, where enhanced screening for Ebola is taking place, would make the determination on who would be quarantined. Anyone who had direct contact with an Ebola patient in Liberia, Sierra Leone or Guinea will be quarantined. In addition, anyone who traveled there but had no such contact would be actively monitored and possibly quarantined, authorities said. This news came a day after a doctor who had treated Ebola patients in Guinea was diagnosed in Manhattan, becoming the fourth person diagnosed with the virus in the United States and the first outside of Dallas. And the decision came not long after a health-care worker who had treated Ebola patients arrived at Newark, one of five airports where people traveling from West Africa to the United States are encountering the stricter screening rules.\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(text): \n",
    "    print(sent)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medical', 'personnel', 'returning', 'to', 'New', 'York', 'and', 'New', 'Jersey', 'from', 'the', 'Ebola', '-', 'riddled', 'countries', 'in', 'West', 'Africa', 'will', 'be', 'automatically', 'quarantined', 'if', 'they', 'had', 'direct', 'contact', 'with', 'an', 'infected', 'person', ',', 'officials', 'announced', 'Friday', '.']\n",
      "\n",
      "['New', 'York', 'Gov', '.']\n",
      "\n",
      "['Andrew', 'Cuomo', '(', 'D', ')', 'and', 'New', 'Jersey', 'Gov', '.']\n",
      "\n",
      "['Chris', 'Christie', '(', 'R', ')', 'announced', 'the', 'decision', 'at', 'a', 'joint', 'news', 'conference', 'Friday', 'at', '7', 'World', 'Trade', 'Center', '.']\n",
      "\n",
      "['“', 'We', 'have', 'to', 'do', 'more', ',”', 'Cuomo', 'said', '.']\n",
      "\n",
      "['“', 'It', '’', 's', 'too', 'serious', 'of', 'a', 'situation', 'to', 'leave', 'it', 'to', 'the', 'honor', 'system', 'of', 'compliance', '.”', 'They', 'said', 'that', 'public', '-', 'health', 'officials', 'at', 'John', 'F', '.', 'Kennedy', 'and', 'Newark', 'Liberty', 'international', 'airports', ',', 'where', 'enhanced', 'screening', 'for', 'Ebola', 'is', 'taking', 'place', ',', 'would', 'make', 'the', 'determination', 'on', 'who', 'would', 'be', 'quarantined', '.']\n",
      "\n",
      "['Anyone', 'who', 'had', 'direct', 'contact', 'with', 'an', 'Ebola', 'patient', 'in', 'Liberia', ',', 'Sierra', 'Leone', 'or', 'Guinea', 'will', 'be', 'quarantined', '.']\n",
      "\n",
      "['In', 'addition', ',', 'anyone', 'who', 'traveled', 'there', 'but', 'had', 'no', 'such', 'contact', 'would', 'be', 'actively', 'monitored', 'and', 'possibly', 'quarantined', ',', 'authorities', 'said', '.']\n",
      "\n",
      "['This', 'news', 'came', 'a', 'day', 'after', 'a', 'doctor', 'who', 'had', 'treated', 'Ebola', 'patients', 'in', 'Guinea', 'was', 'diagnosed', 'in', 'Manhattan', ',', 'becoming', 'the', 'fourth', 'person', 'diagnosed', 'with', 'the', 'virus', 'in', 'the', 'United', 'States', 'and', 'the', 'first', 'outside', 'of', 'Dallas', '.']\n",
      "\n",
      "['And', 'the', 'decision', 'came', 'not', 'long', 'after', 'a', 'health', '-', 'care', 'worker', 'who', 'had', 'treated', 'Ebola', 'patients', 'arrived', 'at', 'Newark', ',', 'one', 'of', 'five', 'airports', 'where', 'people', 'traveling', 'from', 'West', 'Africa', 'to', 'the', 'United', 'States', 'are', 'encountering', 'the', 'stricter', 'screening', 'rules', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(list(nltk.wordpunct_tokenize(sent)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Medical', 'JJ'), ('personnel', 'NNS'), ('returning', 'VBG'), ('to', 'TO'), ('New', 'NNP'), ('York', 'NNP'), ('and', 'CC'), ('New', 'NNP'), ('Jersey', 'NNP'), ('from', 'IN'), ('the', 'DT'), ('Ebola-riddled', 'JJ'), ('countries', 'NNS'), ('in', 'IN'), ('West', 'NNP'), ('Africa', 'NNP'), ('will', 'MD'), ('be', 'VB'), ('automatically', 'RB'), ('quarantined', 'VBN'), ('if', 'IN'), ('they', 'PRP'), ('had', 'VBD'), ('direct', 'JJ'), ('contact', 'NN'), ('with', 'IN'), ('an', 'DT'), ('infected', 'JJ'), ('person', 'NN'), (',', ','), ('officials', 'NNS'), ('announced', 'VBD'), ('Friday', 'NNP'), ('.', '.')]\n",
      "\n",
      "[('New', 'NNP'), ('York', 'NNP'), ('Gov', 'NNP'), ('.', '.')]\n",
      "\n",
      "[('Andrew', 'NNP'), ('Cuomo', 'NNP'), ('(', '('), ('D', 'NNP'), (')', ')'), ('and', 'CC'), ('New', 'NNP'), ('Jersey', 'NNP'), ('Gov', 'NNP'), ('.', '.')]\n",
      "\n",
      "[('Chris', 'NNP'), ('Christie', 'NNP'), ('(', '('), ('R', 'NNP'), (')', ')'), ('announced', 'VBD'), ('the', 'DT'), ('decision', 'NN'), ('at', 'IN'), ('a', 'DT'), ('joint', 'JJ'), ('news', 'NN'), ('conference', 'NN'), ('Friday', 'NNP'), ('at', 'IN'), ('7', 'CD'), ('World', 'NNP'), ('Trade', 'NNP'), ('Center', 'NNP'), ('.', '.')]\n",
      "\n",
      "[('“We', 'NNS'), ('have', 'VBP'), ('to', 'TO'), ('do', 'VB'), ('more', 'JJR'), (',', ','), ('”', 'NNP'), ('Cuomo', 'NNP'), ('said', 'VBD'), ('.', '.')]\n",
      "\n",
      "[('“It’s', 'VB'), ('too', 'RB'), ('serious', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('situation', 'NN'), ('to', 'TO'), ('leave', 'VB'), ('it', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('honor', 'NN'), ('system', 'NN'), ('of', 'IN'), ('compliance.”', 'NN'), ('They', 'PRP'), ('said', 'VBD'), ('that', 'IN'), ('public-health', 'NN'), ('officials', 'NNS'), ('at', 'IN'), ('John', 'NNP'), ('F.', 'NNP'), ('Kennedy', 'NNP'), ('and', 'CC'), ('Newark', 'NNP'), ('Liberty', 'NNP'), ('international', 'JJ'), ('airports', 'NNS'), (',', ','), ('where', 'WRB'), ('enhanced', 'VBN'), ('screening', 'NN'), ('for', 'IN'), ('Ebola', 'NNP'), ('is', 'VBZ'), ('taking', 'VBG'), ('place', 'NN'), (',', ','), ('would', 'MD'), ('make', 'VB'), ('the', 'DT'), ('determination', 'NN'), ('on', 'IN'), ('who', 'WP'), ('would', 'MD'), ('be', 'VB'), ('quarantined', 'VBN'), ('.', '.')]\n",
      "\n",
      "[('Anyone', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('direct', 'JJ'), ('contact', 'NN'), ('with', 'IN'), ('an', 'DT'), ('Ebola', 'NNP'), ('patient', 'NN'), ('in', 'IN'), ('Liberia', 'NNP'), (',', ','), ('Sierra', 'NNP'), ('Leone', 'NNP'), ('or', 'CC'), ('Guinea', 'NNP'), ('will', 'MD'), ('be', 'VB'), ('quarantined', 'VBN'), ('.', '.')]\n",
      "\n",
      "[('In', 'IN'), ('addition', 'NN'), (',', ','), ('anyone', 'NN'), ('who', 'WP'), ('traveled', 'VBD'), ('there', 'RB'), ('but', 'CC'), ('had', 'VBD'), ('no', 'DT'), ('such', 'JJ'), ('contact', 'NN'), ('would', 'MD'), ('be', 'VB'), ('actively', 'RB'), ('monitored', 'VBN'), ('and', 'CC'), ('possibly', 'RB'), ('quarantined', 'VBD'), (',', ','), ('authorities', 'NNS'), ('said', 'VBD'), ('.', '.')]\n",
      "\n",
      "[('This', 'DT'), ('news', 'NN'), ('came', 'VBD'), ('a', 'DT'), ('day', 'NN'), ('after', 'IN'), ('a', 'DT'), ('doctor', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('treated', 'VBN'), ('Ebola', 'NNP'), ('patients', 'NNS'), ('in', 'IN'), ('Guinea', 'NNP'), ('was', 'VBD'), ('diagnosed', 'VBN'), ('in', 'IN'), ('Manhattan', 'NNP'), (',', ','), ('becoming', 'VBG'), ('the', 'DT'), ('fourth', 'JJ'), ('person', 'NN'), ('diagnosed', 'VBD'), ('with', 'IN'), ('the', 'DT'), ('virus', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('and', 'CC'), ('the', 'DT'), ('first', 'JJ'), ('outside', 'NN'), ('of', 'IN'), ('Dallas', 'NNP'), ('.', '.')]\n",
      "\n",
      "[('And', 'CC'), ('the', 'DT'), ('decision', 'NN'), ('came', 'VBD'), ('not', 'RB'), ('long', 'RB'), ('after', 'IN'), ('a', 'DT'), ('health-care', 'JJ'), ('worker', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('treated', 'VBN'), ('Ebola', 'NNP'), ('patients', 'NNS'), ('arrived', 'VBD'), ('at', 'IN'), ('Newark', 'NNP'), (',', ','), ('one', 'CD'), ('of', 'IN'), ('five', 'CD'), ('airports', 'NNS'), ('where', 'WRB'), ('people', 'NNS'), ('traveling', 'VBG'), ('from', 'IN'), ('West', 'NNP'), ('Africa', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('are', 'VBP'), ('encountering', 'VBG'), ('the', 'DT'), ('stricter', 'NN'), ('screening', 'NN'), ('rules', 'NNS'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(list(nltk.pos_tag(nltk.word_tokenize(sent))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these taggers work _pretty_ well - but you can (and should train them on your own corpora). \n",
    "\n",
    "<a id='lemmatize'></a>\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "We have an immense number of word forms as you can see from our various counts in the `FreqDist` above - it is helpful for many applications to normalize these word forms (especially applications like search) into some canonical word for further exploration. In English (and many other languages) - morphological context indicate gender, tense, quantity, etc. but these sublties might not be necessary:\n",
    "\n",
    "<a id='stemming'></a>\n",
    "Stemming = chop off affixes to get the root stem of the word:\n",
    "\n",
    "    running --> run\n",
    "    flowers --> flower\n",
    "    geese   --> geese \n",
    "    \n",
    "Lemmatization = look up word form in a lexicon to get canonical lemma\n",
    "\n",
    "    women   --> woman\n",
    "    foxes   --> fox\n",
    "    sheep   --> sheep\n",
    "    \n",
    "There are several stemmers available:\n",
    "\n",
    "    - Lancaster (English, newer and aggressive)\n",
    "    - Porter (English, original stemmer)\n",
    "    - Snowball (Many languages, newest)\n",
    "    \n",
    "<a id='wordnet'></a>    \n",
    "The Lemmatizer uses the WordNet lexicon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the women run in the fog pass bunni work as comput scientist .\n",
      "the wom run in the fog pass bunny work as comput sci .\n",
      "The women run in the fog pass bunni work as comput scientist .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman running in the fog passed bunny working a computer scientist .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Note: use part of speech tag, we'll see this in machine learning! \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the lemmatizer has to load the WordNet corpus which takes a bit.\n",
    "\n",
    "Typical normalization of text for use as features in machine learning models looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eagle', 'fly', 'at', 'midnight', '.']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    for token in nltk.wordpunct_tokenize(text):\n",
    "        #if you're going to do part of speech tagging, do it here\n",
    "        token = token.lower()\n",
    "        if token in stopwords and token in punctuation:\n",
    "            continue\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        yield token\n",
    "\n",
    "print(list(normalize(\"The eagle flies at midnight.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nerc'></a>\n",
    "## Named Entity Recognition\n",
    "\n",
    "NLTK has an excellent MaxEnt backed Named Entity Recognizer that is trained on the Penn Treebank. You can also retrain the chunker if you'd like - the code is very readable to extend it with a Gazette or otherwise.\n",
    "\n",
    "<a id='chunk'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  (PERSON Smith/NNP)\n",
      "  is/VBZ\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  of/IN\n",
      "  (GPE America/NNP)\n",
      "  and/CC\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP Research/NNP Labs/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(\"John Smith is from the United States of America and works at Microsoft Research Labs\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also wrap the Stanford NER system, which many of you are also probably used to using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PERSON] John\n",
      "[PERSON] Bengfort\n",
      "[O] is\n",
      "[O] from\n",
      "[O] the\n",
      "[LOCATION] United\n",
      "[LOCATION] States\n",
      "[LOCATION] of\n",
      "[LOCATION] America\n",
      "[O] and\n",
      "[O] works\n",
      "[O] at\n",
      "[ORGANIZATION] Microsoft\n",
      "[ORGANIZATION] Research\n",
      "[ORGANIZATION] Labs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# change the paths below to point to wherever you unzipped the Stanford NER download file\n",
    "stanford_root = '/Users/benjamin/Development/stanford-ner-2014-01-04'\n",
    "stanford_data = os.path.join(stanford_root, 'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "stanford_jar  = os.path.join(stanford_root, 'stanford-ner-2014-01-04.jar')\n",
    "\n",
    "st = StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')\n",
    "for i in st.tag(\"John Bengfort is from the United States of America and works at Microsoft Research Labs\".split()):\n",
    "    print('[' + i[1] + '] ' + i[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
