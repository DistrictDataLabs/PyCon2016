{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as data scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practioners have benefitted from machine learning techniques to unlock meaning from large corpora, and in this tutorial we’ll explore how to do that using Python, the Natural Language Toolkit (NLTK) and Gensim. \n",
    "\n",
    "NLTK is an excellent library for machine-learning based NLP, written in Python by experts from both academia and industry. Python allows you to create rich data applications rapidly, iterating on hypotheses. The combination of Python + NLTK means that you can easily add language-aware data products to your larger analytical workflows and applications. \n",
    "\n",
    "## Quick Overview of NLTK\n",
    "NLTK was written by two eminent computational linguists, Steven Bird (Senior Research Associate of the LDC and professor at the University of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). The NTLK library provides a combination of natural language corpora, lexical resources, and example grammars with language processing algorithms, methodologies and demonstrations for a very Pythonic \"batteries included\" view of natural language processing.   \n",
    "\n",
    "As such, NLTK is perfect for research-driven (hypothesis-driven) workflows for agile data science. \n",
    "\n",
    "### Installing NLTK\n",
    "\n",
    "This notebook has a few dependencies, most of which can be installed via the python package manger - `pip`. \n",
    "\n",
    "1. Python 2.7+ or 3.5+ (Anaconda is ok)\n",
    "2. NLTK\n",
    "3. The NLTK corpora \n",
    "4. The BeautifulSoup library\n",
    "5. The gensim libary\n",
    "\n",
    "Once you have Python and pip installed you can install NLTK from the terminal as follows:\n",
    "\n",
    "```bash\n",
    "~$ pip install nltk\n",
    "~$ pip install matplotlib\n",
    "~$ pip install beautifulsoup4\n",
    "~$ pip install gensim\n",
    "```\n",
    "\n",
    "Note that these will also install Numpy and Scipy if they aren't already installed. \n",
    "\n",
    "### What NLTK Includes\n",
    "\n",
    "- [tokenization](#tokenize), [stemming](#stemming), and [tagging](#pos)\n",
    "- [chunking](#chunk) and [parsing](#parse)\n",
    "- language modeling\n",
    "- [classification](#classify) and clustering\n",
    "- logical semantics\n",
    "\n",
    "NLTK is a useful pedagogical resource for learning NLP with Python and serves as a starting place for producing production grade code that requires natural language analysis. It is also important to understand what NLTK is _not_.\n",
    "\n",
    "### What NLTK is _Not_\n",
    "\n",
    "- Production ready out of the box\n",
    "- Lightweight\n",
    "- Generally applicable\n",
    "- Magic\n",
    "\n",
    "NLTK provides a variety of tools that can be used to explore the linguistic domain but is not a lightweight dependency that can be easily included in other workflows, especially those that require unit and integration testing or other build processes. This stems from the fact that NLTK includes a lot of added code but also a rich and complete library of corpora that power the built-in algorithms. \n",
    "\n",
    "### The Good Parts of NLTK\n",
    "\n",
    "- Preprocessing\n",
    "    - [segmentation](#segment)\n",
    "    - [tokenization](#tokenize)\n",
    "    - [PoS tagging](#pos)\n",
    "- Word level processing\n",
    "    - [WordNet](#wordnet)\n",
    "    - [Lemmatization](#lemmatize)\n",
    "    - [Stemming](#stemming)\n",
    "    - [NGrams](#ngram)\n",
    "- Utilities\n",
    "    - Tree\n",
    "    - [FreqDist](#freqdist)\n",
    "    - ConditionalFreqDist\n",
    "    - Streaming CorpusReaders\n",
    "- [Classification](#classify)\n",
    "    - Maximum Entropy\n",
    "    - Naive Bayes\n",
    "    - Decision Tree\n",
    "- [Chunking](#chunk)\n",
    "- [Named Entity Recognition](#nerc)\n",
    "- [Parsers Galore!](#parse)\n",
    "\n",
    "### The Bad parts of NLTK\n",
    "\n",
    "- Syntactic Parsing\n",
    "\n",
    "    - No included grammar (not a black box)\n",
    "    - No Feature/Dependency Parsing\n",
    "    - No included feature grammar\n",
    "\n",
    "- The sem package\n",
    "    \n",
    "    - Toy only (lambda-calculus & first order logic)\n",
    "\n",
    "- Lots of extra stuff (heavyweight dependency)\n",
    "\n",
    "    - papers, chat programs, alignments, etc.\n",
    "\n",
    "Knowing the good and the bad parts will help you explore NLTK further - looking into the source code to extract the material you need, then moving that code to production. We will explore NLTK in more detail in the rest of this notebook. \n",
    "\n",
    "## Obtaining and Exploring the NLTK Corpora\n",
    "NLTK ships with a variety of corpora, let's use a few of them to do some work. To download the NLTK corpora, open a Python interpreter:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "This will open up a window with which you can download the various corpora and models to a specified location. For now, go ahead and download it all as we will be exploring as much of NLTK as we can. Also take note of the `download_directory` - you're going to want to know where that is so you can get a detailed look at the corpora that's included. I usually export an environment variable to track this. You can do this from your terminal:\n",
    "\n",
    "    ~$ export NLTK_DATA=/path/to/nltk_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " u'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " u'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " u'ConditionalFreqDist',\n",
       " u'ConditionalProbDist',\n",
       " u'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " u'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " u'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " u'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " u'DependencyProduction',\n",
       " u'DictionaryConditionalProbDist',\n",
       " u'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " u'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " u'FeatDict',\n",
       " u'FeatList',\n",
       " u'FeatStruct',\n",
       " u'FeatStructReader',\n",
       " u'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " u'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " u'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " u'ImmutableMultiParentedTree',\n",
       " u'ImmutableParentedTree',\n",
       " u'ImmutableProbabilisticMixIn',\n",
       " u'ImmutableProbabilisticTree',\n",
       " u'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " u'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " u'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " u'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " u'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " u'MultiParentedTree',\n",
       " u'MutableProbDist',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " u'Nonterminal',\n",
       " 'OrderedDict',\n",
       " u'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " u'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " u'ProbDistI',\n",
       " u'ProbabilisticDependencyGrammar',\n",
       " u'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " u'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " u'ProbabilisticTree',\n",
       " u'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RandomChartParser',\n",
       " u'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " u'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " u'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " u'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordTagger',\n",
       " 'StanfordTokenizer',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " u'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " u'Text',\n",
       " 'TextCat',\n",
       " u'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " u'TokenSearcher',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " u'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " u'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " u'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'accuracy',\n",
       " u'add_logs',\n",
       " 'agreement',\n",
       " 'alignment_error_rate',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " u'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'class_types',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " u'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'elementtree_indent',\n",
       " u'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'in_idle',\n",
       " u'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " u'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nltk.corpus',\n",
       " 'nonprojectivedependencyparser',\n",
       " u'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_function',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'python_2_unicode_compatible',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " u'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " u'sinica_parse',\n",
       " 'six',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'string_types',\n",
       " 'subprocess',\n",
       " u'subsumes',\n",
       " u'sum_logs',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'text',\n",
       " 'text_type',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " u'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to explore what is in this directory\n",
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for Working with Sample NLTK Corpora\n",
    "\n",
    "To explore much of the built-in corpus, use the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "alpino\n",
      "brown\n",
      "cess_cat\n",
      "cess_esp\n",
      "cmudict\n",
      "comparative_sentences\n",
      "comtrans\n",
      "conll2000\n",
      "conll2002\n",
      "conll2007\n",
      "crubadan\n",
      "demo\n",
      "dependency_treebank\n",
      "find_corpus_fileids\n",
      "floresta\n",
      "framenet\n",
      "gazetteers\n",
      "genesis\n",
      "gutenberg\n",
      "ieer\n",
      "inaugural\n",
      "indian\n",
      "ipipan\n",
      "jeita\n",
      "knbc\n",
      "lin_thesaurus\n",
      "mac_morpho\n",
      "machado\n",
      "masc_tagged\n",
      "movie_reviews\n",
      "multext_east\n",
      "names\n",
      "nkjp\n",
      "nombank\n",
      "nombank_ptb\n",
      "nps_chat\n",
      "opinion_lexicon\n",
      "panlex_lite\n",
      "pl196x\n",
      "ppattach\n",
      "product_reviews_1\n",
      "product_reviews_2\n",
      "propbank\n",
      "propbank_ptb\n",
      "pros_cons\n",
      "ptb\n",
      "qc\n",
      "re\n",
      "reader\n",
      "reuters\n",
      "rte\n",
      "semcor\n",
      "senseval\n",
      "sentence_polarity\n",
      "sentiwordnet\n",
      "shakespeare\n",
      "sinica_treebank\n",
      "state_union\n",
      "stopwords\n",
      "subjectivity\n",
      "swadesh\n",
      "swadesh110\n",
      "swadesh207\n",
      "switchboard\n",
      "tagged_treebank_para_block_reader\n",
      "teardown_module\n",
      "timit\n",
      "timit_tagged\n",
      "toolbox\n",
      "treebank\n",
      "treebank_chunk\n",
      "treebank_raw\n",
      "twitter_samples\n",
      "udhr\n",
      "udhr2\n",
      "universal_treebanks\n",
      "util\n",
      "verbnet\n",
      "webtext\n",
      "wordnet\n",
      "wordnet_ic\n",
      "words\n",
      "ycoe\n"
     ]
    }
   ],
   "source": [
    "# Lists the various corpora and CorpusReader classes in the nltk.corpus module\n",
    "for name in dir(nltk.corpus):\n",
    "    if name.islower() and not name.startswith('_'): print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fileids()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# You can explore the titles with:\n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a_and_c.xml', u'dream.xml', u'hamlet.xml', u'j_caesar.xml', u'macbeth.xml', u'merchant.xml', u'othello.xml', u'r_and_j.xml']\n"
     ]
    }
   ],
   "source": [
    "# For a specific corpus, list the fileids that are available:\n",
    "print(nltk.corpus.shakespeare.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `text.Text()`\n",
    "\n",
    "The `nltk.text.Text` class is a wrapper around a sequence of simple (string) tokens - intended only for _the initial exploration of text_ usually via the Python REPL. It has the following methods:\n",
    "\n",
    "- common_contexts\n",
    "- concordance\n",
    "- collocations\n",
    "- count\n",
    "- plot\n",
    "- findall\n",
    "- index\n",
    "\n",
    "You shouldn't use this class in production level systems, but it is useful to explore (small) snippets of text in a meaningful fashion. \n",
    "\n",
    "For example, you can get access to the text from Hamlet as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamlet = nltk.text.Text(nltk.corpus.gutenberg.words('shakespeare-hamlet.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `concordance()`\n",
    "The concordance function performs a search for the given token and then also provides the surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 172 matches:\n",
      "elfe Bar . Long liue the King Fran . Barnardo ? Bar . \n",
      "e same figure , like the King that ' s dead Mar . Thou\n",
      ". Lookes it not like the King ? Marke it Horatio Hora \n",
      "Mar . Is it not like the King ? Hor . As thou art to t\n",
      "isper goes so : Our last King , Whose Image euen but n\n",
      "mpetent Was gaged by our King : which had return ' d T\n",
      "Secunda . Enter Claudius King of Denmarke , Gertrude t\n",
      "elia , Lords Attendant . King . Though yet of Hamlet o\n",
      "er To businesse with the King , more then the scope Of\n",
      " , will we shew our duty King . We doubt it nothing , \n"
     ]
    }
   ],
   "source": [
    "hamlet.concordance(\"king\", 55, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `similar()`\n",
    "\n",
    "Given some context surrounding a word, we can discover similar words, e.g. words that that occur frequently in the same context and with a similar distribution: Distributional similarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funerall clouds wrath forme that paris eare faculty it together heauen\n",
      "greefe thewes t time loue reputation\n",
      "None\n",
      "\n",
      "mother family side brother life head affection time conduct behaviour\n",
      "feelings visit return house care engagement eyes mind regard arrival\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hamlet.similar(\"marriage\"))\n",
    "austen = nltk.text.Text(nltk.corpus.gutenberg.words(\"austen-sense.txt\"))\n",
    "print()\n",
    "print(austen.similar(\"marriage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this takes a bit of time to build the index in memory, one of the reasons it's not suggested to use this class in production code. \n",
    "\n",
    "#### `common_contexts()`\n",
    "\n",
    "Now that we can do searching and similarity, we find the common contexts of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_that my_and\n"
     ]
    }
   ],
   "source": [
    "hamlet.common_contexts([\"king\", \"father\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your turn, go ahead and explore similar words and contexts - what does the common context mean?_\n",
    "\n",
    "#### `dispersion_plot()`\n",
    "\n",
    "NLTK also uses matplotlib and pylab to display graphs and charts that can show dispersions and frequency. This is especially interesting for the corpus of innagural addresses given by U.S. presidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAEZCAYAAAApEwoTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHxxEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b50ff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inaugural = nltk.text.Text(nltk.corpus.inaugural.words())\n",
    "inaugural.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duty\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'danish', u'dutch', u'english', u'finnish', u'french', u'german', u'hungarian', u'italian', u'norwegian', u'portuguese', u'russian', u'spanish', u'swedish', u'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.fileids())\n",
    "nltk.corpus.stopwords.words('english')\n",
    "import string\n",
    "# print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These corpora export several vital methods:\n",
    "\n",
    "- paras (iterate through each paragraph)\n",
    "- sents (iterate through each sentence)\n",
    "- words (iterate through each word)\n",
    "- raw   (get access to the raw text)\n",
    "\n",
    "#### `paras()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.']], [[u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.']], ...]\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.brown\n",
    "print(corpus.paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sents()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.'], [u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `words()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `raw()`\n",
    "\n",
    "Be careful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' tha\n"
     ]
    }
   ],
   "source": [
    "print(corpus.raw()[:200]) # Be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your turn! Explore some of the text in the available corpora_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='freqdist'></a>\n",
    "## Frequency Analyses \n",
    "\n",
    "In statistical machine learning approaches to NLP, the very first thing we need to do is count things - especially the unigrams that appear in the text and their relationships to each other. NLTK provides two very excellent classes to enable these frequency analyses:\n",
    "\n",
    "- `FreqDist`\n",
    "- `ConditionalFreqDist` \n",
    "\n",
    "And these two classes serve as the foundation for most of the probability and statistical analyses that we will conduct.\n",
    "\n",
    "First we will compute the following:\n",
    "\n",
    "- The count of words\n",
    "- The vocabulary (unique words)\n",
    "- The lexical diversity (the ratio of word count to vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-2233a4e426f2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-2233a4e426f2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print()\"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv))\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "reuters = nltk.corpus.reuters # Corpus of news articles\n",
    "counts  = nltk.FreqDist(reuters.words())\n",
    "vocab   = len(counts.keys())\n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print()\"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConditionalFreqDist' object has no attribute 'B'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-9a34a8f4eeae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConditionalFreqDist' object has no attribute 'B'"
     ]
    }
   ],
   "source": [
    "counts.B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-e3a9a40e7f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The n most common tokens in the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(counts.most_common(40))  # The n most common tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-66cfdd61cfb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The most frequent token in the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(counts.max()) # The most frequent token in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-eead7a6a0bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# A list of all hapax legomena\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "print(counts.hapaxes()[0:10])  # A list of all hapax legomena "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-4ed1c7537414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stipulate'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# percentage of the corpus for this token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "counts.freq('stipulate') * 100 # percentage of the corpus for this token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-2cf2f1bf370f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumulative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "counts.plot(200, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-22cc9866f10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumulative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "counts.plot(200, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "![Zipf's distribution](images/Zipf_distribution_PMF.png)\n",
    "\n",
    "Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. Read more on [Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mystery: 6982 types with 57169 tokens and lexical diversity of 8.188\n",
      "belles_lettres: 18421 types with 173096 tokens and lexical diversity of 9.397\n",
      "humor: 5017 types with 21695 tokens and lexical diversity of 4.324\n",
      "government: 8181 types with 70117 tokens and lexical diversity of 8.571\n",
      "fiction: 9302 types with 68488 tokens and lexical diversity of 7.363\n",
      "reviews: 8626 types with 40704 tokens and lexical diversity of 4.719\n",
      "religion: 6373 types with 39399 tokens and lexical diversity of 6.182\n",
      "romance: 8452 types with 70022 tokens and lexical diversity of 8.285\n",
      "science_fiction: 3233 types with 14470 tokens and lexical diversity of 4.476\n",
      "adventure: 8874 types with 69342 tokens and lexical diversity of 7.814\n",
      "editorial: 9890 types with 61604 tokens and lexical diversity of 6.229\n",
      "hobbies: 11935 types with 82345 tokens and lexical diversity of 6.899\n",
      "lore: 14503 types with 110299 tokens and lexical diversity of 7.605\n",
      "news: 14394 types with 100554 tokens and lexical diversity of 6.986\n",
      "learned: 16859 types with 181888 tokens and lexical diversity of 10.789\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain \n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "categories = brown.categories()\n",
    "\n",
    "counts = nltk.ConditionalFreqDist(chain(*[[(cat, word) for word in brown.words(categories=cat)] for cat in categories]))\n",
    "\n",
    "for category, dist in counts.items():\n",
    "    vocab  = len(dist.keys())\n",
    "    tokens = sum(dist.values())\n",
    "    lexdiv = float(tokens) / float(vocab)\n",
    "    print(\"%s: %i types with %i tokens and lexical diversity of %0.3f\" % (category, vocab, tokens, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your turn: compute the conditional frequency distribution of bigrams in a corpus_\n",
    "\n",
    "Hint:\n",
    "\n",
    "<a id=ngram></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ngram in nltk.ngrams([\"The\", \"bear\", \"walked\", \"in\", \"the\", \"woods\", \"at\", \"midnight\"], 5):\n",
    "    print(ngram)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "\n",
    "NLTK is great at the preprocessing of Raw text - it provides the following tools for dividing text into it's constituent parts:\n",
    "<a id='tokenize'></a>\n",
    "<a id='segment'></a>\n",
    "- `sent_tokenize`: a Punkt sentence tokenizer:\n",
    "\n",
    "    This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.  It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "    \n",
    "    However, Punkt is designed to learn parameters (a list of abbreviations, etc.) unsupervised from a corpus similar to the target domain. The pre-packaged models may therefore be unsuitable: use PunktSentenceTokenizer(text) to learn parameters from the given text.\n",
    "    \n",
    "    \n",
    "- `word_tokenize`: a Treebank tokenizer \n",
    "\n",
    "    The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This is the method that is invoked by ``word_tokenize()``.  It assumes that the text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
    "    \n",
    "<a id='pos'></a>\n",
    "- `pos_tag`: a maximum entropy tagger trained on the Penn Treebank\n",
    "\n",
    "    There are several other taggers including (notably) the BrillTagger as well as the BrillTrainer to train your own tagger or tagset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = u\"Medical personnel returning to New York and New Jersey from the Ebola-riddled countries in West Africa will be automatically quarantined if they had direct contact with an infected person, officials announced Friday. New York Gov. Andrew Cuomo (D) and New Jersey Gov. Chris Christie (R) announced the decision at a joint news conference Friday at 7 World Trade Center. “We have to do more,” Cuomo said. “It’s too serious of a situation to leave it to the honor system of compliance.” They said that public-health officials at John F. Kennedy and Newark Liberty international airports, where enhanced screening for Ebola is taking place, would make the determination on who would be quarantined. Anyone who had direct contact with an Ebola patient in Liberia, Sierra Leone or Guinea will be quarantined. In addition, anyone who traveled there but had no such contact would be actively monitored and possibly quarantined, authorities said. This news came a day after a doctor who had treated Ebola patients in Guinea was diagnosed in Manhattan, becoming the fourth person diagnosed with the virus in the United States and the first outside of Dallas. And the decision came not long after a health-care worker who had treated Ebola patients arrived at Newark, one of five airports where people traveling from West Africa to the United States are encountering the stricter screening rules.\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(text): \n",
    "    print(sent)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(list(nltk.wordpunct_tokenize(sent)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(list(nltk.pos_tag(nltk.word_tokenize(sent))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these taggers work _pretty_ well - but you can (and should train them on your own corpora). \n",
    "\n",
    "<a id='lemmatize'></a>\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "We have an immense number of word forms as you can see from our various counts in the `FreqDist` above - it is helpful for many applications to normalize these word forms (especially applications like search) into some canonical word for further exploration. In English (and many other languages) - morphological context indicate gender, tense, quantity, etc. but these sublties might not be necessary:\n",
    "\n",
    "<a id='stemming'></a>\n",
    "Stemming = chop off affixes to get the root stem of the word:\n",
    "\n",
    "    running --> run\n",
    "    flowers --> flower\n",
    "    geese   --> geese \n",
    "    \n",
    "Lemmatization = look up word form in a lexicon to get canonical lemma\n",
    "\n",
    "    women   --> woman\n",
    "    foxes   --> fox\n",
    "    sheep   --> sheep\n",
    "    \n",
    "There are several stemmers available:\n",
    "\n",
    "    - Lancaster (English, newer and aggressive)\n",
    "    - Porter (English, original stemmer)\n",
    "    - Snowball (Many languages, newest)\n",
    "    \n",
    "<a id='wordnet'></a>    \n",
    "The Lemmatizer uses the WordNet lexicon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the lemmatizer has to load the WordNet corpus which takes a bit.\n",
    "\n",
    "Typical normalization of text for use as features in machine learning models looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    for token in nltk.wordpunct_tokenize(text):\n",
    "        #if you're going to do part of speech tagging, do it here\n",
    "        token = token.lower()\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            continue\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        yield token\n",
    "\n",
    "print(list(normalize(\"The eagle flies at midnight.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nerc'></a>\n",
    "## Named Entity Recognition\n",
    "\n",
    "NLTK has an excellent MaxEnt backed Named Entity Recognizer that is trained on the Penn Treebank. You can also retrain the chunker if you'd like - the code is very readable to extend it with a Gazette or otherwise.\n",
    "\n",
    "<a id='chunk'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(\"John Smith is from the United States of America and works at Microsoft Research Labs\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also wrap the Stanford NER system, which many of you are also probably used to using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# change the paths below to point to wherever you unzipped the Stanford NER download file\n",
    "stanford_root = '/Users/benjamin/Development/stanford-ner-2014-01-04'\n",
    "stanford_data = os.path.join(stanford_root, 'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "stanford_jar  = os.path.join(stanford_root, 'stanford-ner-2014-01-04.jar')\n",
    "\n",
    "st = StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')\n",
    "for i in st.tag(\"John Bengfort is from the United States of America and works at Microsoft Research Labs\".split()):\n",
    "    print('[' + i[1] + '] ' + i[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The primary responsibility you will have before any task involving NLP is to ingest and transform raw text into a corpus that can then be used for performing further evaluations. NLTK provides many corpora for you to work with for exploration, but you must become able to design and construct your own corpora, and to implement `nltk.CorpusReader` objects - classes that in a memory safe and efficient way are able to read entire corpora and analyze them. \n",
    "\n",
    "Many people get away with the `nltk.PlainTextCorpusReader` - which uses built-in taggers and tokenizers to deal with raw text. However, this methodology leaves you at the mercy of the tagging model that you have provided, and does not allow you to make corrections that are saved in between runs. Instead you should preprocess your text to allow it to be read by the `nltk.corpus.TaggedCorpusReader` or the penultimate corpus, the `nltk.corpus.BracketParseCorpusReader`. \n",
    "\n",
    "In this task, you will transform raw text into a format that can then be read by the `nltk.corpus.TaggedCorpusReader`. See the documentation at [http://www.nltk.org/api/nltk.corpus.reader.html](http://www.nltk.org/api/nltk.corpus.reader.html) for more information on this reader. \n",
    "\n",
    "You will find 20-40 documents of recent tech articles from Engadget and Tech Crunch at the following link: [http://bit.ly/nlpnltkcorpus](http://bit.ly/nlpnltkcorpus) - please download them to your local file system. Write a Python program that uses NLTK to preprocess these documents into a format that can be easily read by the `nltk.corpus.TaggedCorpusReader`. \n",
    "\n",
    "Note that you will have to process these files and remove HTML tags and you might have to do other tasks related to the clean up; to do this I suggest you use the third party library BeautifulSoup which can be found at [http://www.crummy.com/software/BeautifulSoup/](http://www.crummy.com/software/BeautifulSoup/). See also Chapter 3 in the NLTK book for more information.\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "1. What is the word count and vocabulary of this corpus?\n",
    "2. What is the lexical diversity of the corpus?\n",
    "3. What are the 5 most common lexical categories (parts of speech)?\n",
    "4. What are the 10 most common unigrams, the 10 most common bigrams? (please exclude stopwords, using the `nltk.corpus.stopwords('english')` list)\n",
    "5. How many nouns are in the corpus?\n",
    "\n",
    "<a id='parse'></a>\n",
    "### Parsing\n",
    "\n",
    "Given a seed inventory of pre-terminal and non-terminal symbols (grammatical categories) and a sample lexicon, write a grammar for English noun phrases. Your grammar should cover all legal structures of noun phrases used by the grammatical categories provided. You must include the following:\n",
    "\n",
    "- noun-noun compounds (\"brick wall\", \"lawn chair\")\n",
    "- relative clauses of the form Rel-Cl ⟶ Rel-Pro V NP (\"[the ball] that hit her\") \n",
    "\n",
    "**Note:** You do not need to cover more than one PP in a row, more than one adjective in a row, noun-noun compounds of length > 2, quantifiers followed by determiners (\"all of these\") or mass nouns (\"beer\", \"sincerity\")\n",
    "\n",
    "You should then write a program that uses an NLTK parser and the grammar you constructed that will return a syntactic tree if the input is a noun phrase or None if the input is ungrammatical. Your program will have to take the input sentence, tokenize it and then tag it according to the lexicon (you can assume that words in this lexicon do not have multiple senses) - you'll then have to pass the grammar phrase (the tags) to the parser.\n",
    "\n",
    "#### Tagset\n",
    "\n",
    "    N = noun\n",
    "    NP = noun phrase\n",
    "    Adj = adjective\n",
    "    AdjP = adjective phrase\n",
    "    Adv = adverb\n",
    "    Prep = preposition\n",
    "    PP = prepositional phrase\n",
    "    Quant = quantifier\n",
    "    Ord = ordinal numeral\n",
    "    Card = cardinal numeral\tRel-Cl = relative clause\n",
    "    Rel-Pro = relative pronoun\n",
    "    V = verb\n",
    "    S = sentence\n",
    "    Det = determiner\n",
    "    Dem-Det = demonstrative determiner\n",
    "    Wh-Det = wh-determiner\n",
    "    PPron = personal pronoun\n",
    "    PoPron = possessive pronoun\n",
    "\n",
    "#### Sample Lexicon\n",
    "\n",
    "    a            Det\n",
    "    an           Det\n",
    "    at           Prep\n",
    "    airplane     NSg\n",
    "    airplanes    NPl\n",
    "    airport      NSg\n",
    "    airports     NPl\n",
    "    any          Quant\n",
    "    beautiful    Adj\n",
    "    big          Adj\n",
    "    eat          V\n",
    "    eats         V3Sg\n",
    "    finished     VPastPP\n",
    "    four         Card\n",
    "    fourth       Ord\n",
    "    he           PPron\n",
    "    his          PoPron\n",
    "    in           Prep\n",
    "    many         Quant\n",
    "    my           PoPron\n",
    "    new          Adj\n",
    "    of           Prep\n",
    "    offered      VPastPP\n",
    "    on           Prep\n",
    "    restaurant   NSg\n",
    "    restaurants  NPl\n",
    "    runway       NSg\n",
    "    runways      NPl\n",
    "    second       Ord\n",
    "    some         Quant\n",
    "    that         Dem-DetSg\n",
    "    that         Rel-Pro\n",
    "    the          Det\n",
    "    this         Dem-DetSg\n",
    "    these        Dem-DetPl\n",
    "    third        Ord\n",
    "    those        Dem-DetPl\n",
    "    three        Card\n",
    "    two          Card\n",
    "    very         Adv\n",
    "    which        Wh-Det\n",
    "    who          Wh-Det\n",
    "    you          PPron\n",
    "\n",
    "#### Evaluation Phrases\n",
    "\n",
    "- \"Four new airports\"\n",
    "- \"Very new airport runways\"\n",
    "- \"His second house\"\n",
    "- \"Some beautiful dishes which a restaurant offered\"\n",
    "- \"The runway that the airport built\"\n",
    "\n",
    "<a id='classify'></a>\n",
    "### Document Classification\n",
    "\n",
    "In the first week you created an ingestion mechanism and an NLTK corpus reader for a set of RSS feeds. These feeds potentially have topics associated with them (broad tags like tech, news, sports, etc). In this question you'll build a classifier on a data set of RSS feeds that is provided in the course materials to decide whether or not you can categorize the various topics using one of the classifiers you learned in this week.\n",
    "\n",
    "The corpus is constructed as follows. Each individual blog post is in its own HTML file stored in a directory labled with the topic. Use the `nltk.CategorizedCorpusReader` or the `nltk.CategorizedPlaintextCorpusReader` to construct your corpora (you may review how the movie reviews data set is structured). To do this you need to pass to the corpus the path to the root of your corpus, and a regular expression to match file names. You also need to use a regular expression passed as the `cat_pattern` keyword argument, which is used to match the category labels. Here is an example for the spam corpus:\n",
    "\n",
    "    from nltk.corpus import CategorizedPlaintextCorpusReader as EmailCorpus\n",
    "\n",
    "    corpus   = EmailCorpus(\"./data/nbspam\", r'(?!\\.).*\\.[a-f0-9]+',\n",
    "                   cat_pattern=r'(spam|ham)/.*', encoding='iso-8859-1')\n",
    "\n",
    "    print(corpus.categories())\n",
    "    print(corpus.fileids())\n",
    "\n",
    "Create a test set, a dev test set, and a training set from randomly shuffled documents that are in the corpus to use in your development. Save these sets to disk with pickles to ensure that you can develop easily with them.\n",
    "\n",
    "Create a function that extracts features per document. Choose any features you would like. One idea is to use the most common unigrams; you might be able to use common bigrams as well. If you can think of any other features, feel free to include them as well (maybe an includes_recipe feature, etc.)) You may want to consider a TF-IDF feature to improve your results.\n",
    "\n",
    "Train the classifier of your choice on the training data, and then improve it with your dev set. Report your final accuracy and the most informative features by running the accuracy checker on the final test set.\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "1. Report accuracy and most informative features of classifier (4 points)\n",
    "2. Show complete work with submitted Python code (6 points)\n",
    "3. Create a corpus reader that extends a built-in NLTK corpus reader (4 points)\n",
    "4. Create an efficient feature extractor (4 points)\n",
    "5. Achieve an accuracy with your classifier of greater than 85% (2 points)\n",
    "\n",
    "### Product Classification\n",
    "\n",
    "The second question involves comparing and contrast the Naive Bayes Classifier with the Maximum Entropy classifier. You will be given an abbreviated data set of product names and their descriptions as well as their label (tops, bottoms, shoes, etc.) - similarly to question one, create a corpus that can read the CSV file - you may want to look at the `nltk.corpus.WordListCorpusReader` for inspiration about how to create such a corpus (each product is on a single line).\n",
    "\n",
    "Create test and training sets from the data then build both a NaiveBayes and Maxent classifier - make sure that you save these classifiers to disk using the `pickle` module! The Maxent classifier in particular will take a long time to train. Once they're trained; report the accuracy of each as well as the most informative features. Are there any surprises? Which classifier performs better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
