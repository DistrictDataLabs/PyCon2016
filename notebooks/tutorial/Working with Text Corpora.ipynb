{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Copora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import bs4\n",
    "import json\n",
    "import nltk\n",
    "import time \n",
    "import codecs\n",
    "import pickle \n",
    "import shutil\n",
    "\n",
    "from six import string_types\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "from readability.readability import Document as Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module Variables\n",
    "ROOT     = os.getcwd() \n",
    "CORPUS   = os.path.join(ROOT, \"fixtures\", \"corpus\")\n",
    "GALACTIC = os.path.join(ROOT, \"fixtures\", \"galactic\")\n",
    "TAGGED   = os.path.join(ROOT, \"fixtures\", \"preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plaintext Corpora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[\\w_\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "CAT_PATTERN = r'([\\w_\\s]+)/.*'\n",
    "\n",
    "corpus = nltk.corpus.reader.plaintext.CategorizedPlaintextCorpusReader(\n",
    "    GALACTIC, DOC_PATTERN, cat_pattern=CAT_PATTERN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorpusView\n",
      "abspath\n",
      "abspaths\n",
      "categories\n",
      "citation\n",
      "encoding\n",
      "ensure_loaded\n",
      "fileids\n",
      "license\n",
      "open\n",
      "paras\n",
      "raw\n",
      "readme\n",
      "root\n",
      "sents\n",
      "unicode_repr\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "# What properties do we get?\n",
    "for prop in dir(corpus):\n",
    "    if not prop.startswith('_'): print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Trek\n",
      "Star Wars\n"
     ]
    }
   ],
   "source": [
    "# Print out the categories \n",
    "for cat in corpus.categories(): print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Trek/Star Trek - Balance of Terror.txt\n",
      "Star Trek/Star Trek - First Contact.txt\n",
      "Star Trek/Star Trek - Generations.txt\n",
      "Star Trek/Star Trek - Nemesis.txt\n",
      "Star Trek/Star Trek - The Motion Picture.txt\n",
      "Star Trek/Star Trek 2 - The Wrath of Khan.txt\n",
      "Star Trek/Star Trek.txt\n",
      "Star Wars/Star Wars Episode 1.txt\n",
      "Star Wars/Star Wars Episode 2.txt\n",
      "Star Wars/Star Wars Episode 3.txt\n",
      "Star Wars/Star Wars Episode 4.txt\n",
      "Star Wars/Star Wars Episode 5.txt\n",
      "Star Wars/Star Wars Episode 6.txt\n",
      "Star Wars/Star Wars Episode 7.txt\n"
     ]
    }
   ],
   "source": [
    "# Print out the fileids\n",
    "for fid in corpus.fileids(): print(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Galactic Classifier\n",
      "\n",
      "Can you spot the difference in language between Star Wars and Star Trek?\n",
      "\n",
      "Scripts obtained from:\n",
      "\n",
      "- http://www.imsdb.com/alphabetical/S\n",
      "- http://www.chakoteya.net/StarTrek/9.htm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the README\n",
    "print(corpus.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (c) by Lucas Arts and Paramount Pictures. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the LICENSE\n",
    "print(corpus.license())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@misc{ddl_galactic_2016,\n",
      "  title = {Galactic {{Corpus}}},\n",
      "  timestamp = {2016-04-19T17:16:23Z},\n",
      "  publisher = {{District Data Labs}},\n",
      "  author = {Voorhees, Will and Bengfort, Benjamin},\n",
      "  month = apr,\n",
      "  year = {2016}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the CITATION\n",
    "print(corpus.citation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STAR', 'TREK']\n",
      "['Written', 'by']\n",
      "['Roberto', 'Orci', '&', 'Alex', 'Kurtzman']\n",
      "['November', ',', '2007']\n",
      "['1', '-', '13', 'OMIT', '1', '-', '13']\n",
      "['14', 'OMIT', '14']\n",
      "['15', 'OVER', 'BLACKNESS', ',', 'we', 'HEAR', 'a', 'BACH', 'HARPSICHORD', 'CONCERTO', '.']\n",
      "['And', 'then', 'a', '15', 'WOMAN', '--', 'breathing', 'hard', '--', 'straining', ',', 'harder', 'and', 'harder', '--', 'until', 'finally', 'we', 'HEAR', 'a', 'NEWBORN', 'BABY', 'CRYING', '--', 'and', 'we', '...']\n",
      "['FADE', 'IN', ':']\n",
      "['EXT', '.']\n",
      "['VULCAN', 'FAMILY', 'HOME', '-', 'DUSK']\n",
      "['The', 'image', 'is', 'spectacular', ',', 'aglow', 'in', 'DUSK', 'LIGHT', ':', 'a', 'beautiful', 'BABY', ',', 'just', 'born', ',', 'held', 'in', 'a', 'WOMAN', \"'\", 'S', 'HANDS', '.']\n",
      "['It', 'is', 'being', 'cleaned', ';', 'warm', 'water', 'runs', 'down', 'its', 'face', 'and', 'body', '.']\n",
      "['TIGHT', 'ON', 'the', 'MIDWIFE', '(', 'female', ',', '60', \"'\", 's', ')', 'who', 'holds', 'and', 'cleans', 'the', 'baby', 'as', 'it', 'CRIES', '.']\n"
     ]
    }
   ],
   "source": [
    "# View the first 12 sentences in Star Trek\n",
    "for idx, sent in enumerate(corpus.sents(fileids='Star Trek/Star Trek.txt')):\n",
    "    print(sent)\n",
    "    if idx > 12: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baleen Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus Patterns \n",
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "\n",
    "class BaleenCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for the raw JSON Baleen documents that have not been\n",
    "    preprocessed and have the complete feed information exported from Mongo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tags to extract as paragraphs from the HTML text\n",
    "    TAGS = [\n",
    "        'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root, fileids=DOC_PATTERN, tags=None,\n",
    "                 word_tokenizer=WordPunctTokenizer(),\n",
    "                 sent_tokenizer=nltk.data.LazyLoader(\n",
    "                    'tokenizers/punkt/english.pickle'),\n",
    "                 encoding='utf8', **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "\n",
    "        self._word_tokenizer = word_tokenizer\n",
    "        self._sent_tokenizer = sent_tokenizer\n",
    "        self._good_tags = tags or self.TAGS\n",
    "\n",
    "    def feeds(self):\n",
    "        \"\"\"\n",
    "        Opens and returns the collection of feeds associated with the corpus.\n",
    "        \"\"\"\n",
    "        data = self.open('feeds.json')\n",
    "        return json.load(data)\n",
    "\n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete JSON document for every file in the corpus.\n",
    "        Note that I attempted to use the nltk ``CorpusView`` and ``concat``\n",
    "        methods here, but was not getting memory safe iteration. Instead the\n",
    "        simple Python generator by far did a better job of ensuring that file\n",
    "        handles got closed and that not all data was loaded into memory at a\n",
    "        time. In the future, I will try to re-implement the corpus view.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with codecs.open(path, 'r', encoding=enc) as f:\n",
    "                yield json.load(f)\n",
    "\n",
    "    def fields(self, fields, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Helper function to extract particular fields from the json documents.\n",
    "        Fields can be a string or an iterable of fields. If just one field is\n",
    "        passed in, then the values are returned, otherwise dictionaries of\n",
    "        the requsted fields are returned.\n",
    "\n",
    "        This method doesn't raise KeyErrors nor does it yield None values if\n",
    "        the document doesn't contain a particular field.\n",
    "\n",
    "        For example to get title and pubdate from the documents:\n",
    "\n",
    "            corpus.fields(['title', 'pubdate'])\n",
    "\n",
    "        Or to simply get all of the summaries:\n",
    "\n",
    "            corpus.fields('summary')\n",
    "\n",
    "        Note: there is not yet support for nested fields.\n",
    "        \"\"\"\n",
    "        if isinstance(fields, string_types):\n",
    "            fields = [fields,]\n",
    "\n",
    "        if len(fields) == 1:\n",
    "            for doc in self.docs(fileids, categories):\n",
    "                if fields[0] in doc:\n",
    "                    yield doc[fields[0]]\n",
    "\n",
    "        else:\n",
    "            for doc in self.docs(fileids, categories):\n",
    "                yield {\n",
    "                    key: doc.get(key, None)\n",
    "                    for key in fields\n",
    "                }\n",
    "\n",
    "    def html(self, fileids=None, categories=None, readability=True):\n",
    "        \"\"\"\n",
    "        Returns the HTML content from each JSON document for every file in\n",
    "        the corpus, ensuring that it exists. Note, this simply returns the\n",
    "        HTML strings, it doesn't perform any parsing of the HTML.\n",
    "\n",
    "        If readability is True, clean HTML is returned.\n",
    "        \"\"\"\n",
    "        ## Returns a generator of documents.\n",
    "        html = self.fields('content', fileids, categories)\n",
    "        if readability:\n",
    "            for doc in html:\n",
    "                try:\n",
    "                    yield Paper(doc).summary()\n",
    "                except:\n",
    "                    continue\n",
    "        else:\n",
    "            for doc in html:\n",
    "                yield doc\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses BeautifulSoup to parse the paragraphs from the HTML.\n",
    "        Currently, this just sends raw text, it does not do any segmentation\n",
    "        or tokenization as the standard NLTK CorpusReader objects do.\n",
    "        \"\"\"\n",
    "        for html in self.html(fileids, categories):\n",
    "            soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "            for element in soup.find_all(self._good_tags):\n",
    "                yield element.text\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in sentence tokenizer to extract sentences from the\n",
    "        paragraphs. Note that this method uses BeautifulSoup to parse HTML.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in self._sent_tokenizer.tokenize(paragraph):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in word tokenizer to extract tokens from sentences.\n",
    "        Note that this method uses BeautifulSoup to parse HTML content.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for token in self._word_tokenizer.tokenize(sentence):\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baleen corpus contains 24867 files in 12 categories\n"
     ]
    }
   ],
   "source": [
    "# Load the Baleen Corpus Reader\n",
    "corpus = BaleenCorpusReader(CORPUS)\n",
    "print(\n",
    "    \"Baleen corpus contains {} files in {} categories\".format(\n",
    "        len(corpus.fileids()), len(corpus.categories())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Corpora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    The preprocessor wraps a corpus object (usually a `BaleenCorpusReader`)\n",
    "    and manages the stateful tokenization and part of speech tagging into a\n",
    "    directory that is stored in a format that can be read by the\n",
    "    `BaleenPickledCorpusReader`. This format is more compact and necessarily\n",
    "    removes a variety of fields from the document that are stored in the JSON\n",
    "    representation dumped from the Mongo database. This format however is more\n",
    "    easily accessed for common parsing activity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, target=None):\n",
    "        \"\"\"\n",
    "        The corpus is the `BaleenCorpusReader` to preprocess and pickle.\n",
    "        The target is the directory on disk to output the pickled corpus to.\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.target = target\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "\n",
    "    @target.setter\n",
    "    def target(self, path):\n",
    "        if path is not None:\n",
    "            # Normalize the path and make it absolute\n",
    "            path = os.path.expanduser(path)\n",
    "            path = os.path.expandvars(path)\n",
    "            path = os.path.abspath(path)\n",
    "\n",
    "            if os.path.exists(path):\n",
    "                if not os.path.isdir(path):\n",
    "                    raise ValueError(\n",
    "                        \"Please supply a directory to write preprocessed data to.\"\n",
    "                    )\n",
    "\n",
    "        self._target = path\n",
    "\n",
    "    def fileids(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Helper function access the fileids of the corpus\n",
    "        \"\"\"\n",
    "        fileids = self.corpus._resolve(fileids, categories)\n",
    "        if fileids:\n",
    "            return fileids\n",
    "        return self.corpus.fileids()\n",
    "\n",
    "    def abspath(self, fileid):\n",
    "        \"\"\"\n",
    "        Returns the absolute path to the target fileid from the corpus fileid.\n",
    "        \"\"\"\n",
    "        # Find the directory, relative from the corpus root.\n",
    "        parent = os.path.relpath(\n",
    "            os.path.dirname(self.corpus.abspath(fileid)), corpus.root\n",
    "        )\n",
    "\n",
    "        # Compute the name parts to reconstruct\n",
    "        basename  = os.path.basename(fileid)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "\n",
    "        # Create the pickle file extension\n",
    "        basename  = name + '.pickle'\n",
    "\n",
    "        # Return the path to the file relative to the target.\n",
    "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
    "\n",
    "    def tokenize(self, fileid):\n",
    "        \"\"\"\n",
    "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
    "        generator of paragraphs, which are lists of sentences, which in turn\n",
    "        are lists of part of speech tagged words.\n",
    "        \"\"\"\n",
    "        for paragraph in self.corpus.paras(fileids=fileid):\n",
    "            yield [\n",
    "                nltk.pos_tag(nltk.wordpunct_tokenize(sent))\n",
    "                for sent in nltk.sent_tokenize(paragraph)\n",
    "            ]\n",
    "\n",
    "    def process(self, fileid):\n",
    "        \"\"\"\n",
    "        For a single file does the following preprocessing work:\n",
    "\n",
    "            1. Checks the location on disk to make sure no errors occur.\n",
    "            2. Gets all paragraphs for the given text.\n",
    "            3. Segements the paragraphs with the sent_tokenizer\n",
    "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
    "            5. Tags the sentences using the default pos_tagger\n",
    "            6. Writes the document as a pickle to the target location.\n",
    "\n",
    "        This method is called multiple times from the transform runner.\n",
    "        \"\"\"\n",
    "        # Compute the outpath to write the file to.\n",
    "        target = self.abspath(fileid)\n",
    "        parent = os.path.dirname(target)\n",
    "\n",
    "        # Make sure the directory exists\n",
    "        if not os.path.exists(parent):\n",
    "            os.makedirs(parent)\n",
    "\n",
    "        # Make sure that the parent is a directory and not a file\n",
    "        if not os.path.isdir(parent):\n",
    "            raise ValueError(\n",
    "                \"Please supply a directory to write preprocessed data to.\"\n",
    "            )\n",
    "\n",
    "        # Ensure that we are not overwriting existing data\n",
    "        if os.path.exists(target):\n",
    "            raise ValueError(\n",
    "                \"Path at '{}' already exists!\".format(target)\n",
    "            )\n",
    "\n",
    "        # Create a data structure for the pickle\n",
    "        document = list(self.tokenize(fileid))\n",
    "\n",
    "        # Open and serialize the pickle to disk\n",
    "        with open(target, 'wb') as f:\n",
    "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Clean up the document\n",
    "        del document\n",
    "\n",
    "        # Return the target fileid\n",
    "        return target\n",
    "\n",
    "    def transform(self, fileids=None, categories=None, target=None):\n",
    "        \"\"\"\n",
    "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
    "        and part of speech tagged corpus as a pickle to the target directory.\n",
    "\n",
    "        This method will also directly copy files that are in the corpus.root\n",
    "        directory that are not matched by the corpus.fileids()\n",
    "        \"\"\"\n",
    "        # Add the new target directory\n",
    "        if target: self.target = target\n",
    "\n",
    "        # Make the target directory if it doesn't already exist\n",
    "        if not os.path.exists(self.target):\n",
    "            os.makedirs(self.target)\n",
    "\n",
    "        # First shutil.copy anything in the root directory.\n",
    "        names = [\n",
    "            name  for name in os.listdir(self.corpus.root)\n",
    "            if not name.startswith('.')\n",
    "        ]\n",
    "\n",
    "        # Filter out directories and copy files\n",
    "        for name in names:\n",
    "            source = os.path.abspath(os.path.join(self.corpus.root, name))\n",
    "            target = os.path.abspath(os.path.join(self.target, name))\n",
    "\n",
    "            if os.path.isfile(source):\n",
    "                shutil.copy(source, target)\n",
    "\n",
    "        # Resolve the fileids to start processing\n",
    "        fileids = self.fileids(fileids, categories)\n",
    "        return map(self.process, fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "transformer = Preprocessor(corpus, TAGGED)\n",
    "docs = transformer.transform()\n",
    "print(\n",
    "    \"Transformed {} docs in {:0.3f} seconds\".format(\n",
    "        len(list(docs)), time.time() - start\n",
    "    )\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
