{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on Text/Language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk\n",
    "import math\n",
    "import gensim \n",
    "import pickle \n",
    "import random \n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module Variables\n",
    "ROOT   = os.getcwd() \n",
    "CORPUS = os.path.join(ROOT, \"fixtures\", \"tagged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Corpus Reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "class BaleenCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    Quick reader for the preprocessed tokenized and tagged version of the corpus. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, categoryids=CAT_PATTERN):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        CategorizedCorpusReader.__init__(self, {\"cat_pattern\": categoryids})\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "    \n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for token in sentence:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = BaleenCorpusReader(CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print statistics about each category. \n",
    "words = nltk.ConditionalFreqDist([\n",
    "        (category, word) \n",
    "        for category in corpus.categories()\n",
    "        for word in corpus.words(categories=category)\n",
    "    ])\n",
    "\n",
    "for category, dist in words.items():\n",
    "    wc = sum(dist.values())\n",
    "    vb = len(dist) \n",
    "    print(\"{} has {:>,} vocab and {:>,} words\".format(category, vb, wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers \n",
    "\n",
    "### Build Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labeled_documents(corpus, categories=None):\n",
    "    \"\"\"\n",
    "    Returns a list of (document, label) tuples where a document is a \n",
    "    list of (token, tag) pairs and label is the supervised classes.  \n",
    "    \"\"\"\n",
    "    # Get the total list of categories\n",
    "    categories = categories or corpus.categories() \n",
    "        \n",
    "    # Build a list of the documents with their associated words\n",
    "    # Note this loads the entire corpus into memory!\n",
    "    return [\n",
    "        (\n",
    "            list(corpus.words(fileids=fileid)), \n",
    "            corpus.categories(fileids=fileid)[0]   \n",
    "        )\n",
    "        for fileid in corpus.fileids(categories=categories)\n",
    "    ]\n",
    "    \n",
    "\n",
    "def train_test_split(docs, categories=None, test=0.2):\n",
    "    \"\"\"\n",
    "    Build a training and testing set of documents with their associated \n",
    "    labels by shuffling the documents, then returning test% and 1-test%\n",
    "    of the data set (e.g. the test and train sets). \n",
    "    \"\"\"\n",
    "    \n",
    "    # Shuffle the document in place \n",
    "    random.shuffle(docs) \n",
    "    \n",
    "    # Find the split index \n",
    "    split = math.floor(len(docs)*test)\n",
    "    \n",
    "    # Return the train/test based on the split \n",
    "    return docs[split:], docs[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(labeled_documents(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STOPWORDS   = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer  = nltk.WordNetLemmatizer() \n",
    "\n",
    "def is_punct(token):\n",
    "    # Is every character punctuation? \n",
    "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "\n",
    "def wnpos(tag):\n",
    "    # Return the WordNet POS tag from the Penn Treebank tag \n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "\n",
    "def normalize(document, stopwords=STOPWORDS):\n",
    "    \"\"\"\n",
    "    Removes stopwords and punctuation, lowercases, lemmatizes \n",
    "    \"\"\"\n",
    "    \n",
    "    for token, tag in document:\n",
    "        token = token.lower().strip() \n",
    "        \n",
    "        if is_punct(token) or (token in stopwords):\n",
    "            continue \n",
    "        \n",
    "        yield lemmatizer.lemmatize(token, wnpos(tag))\n",
    "       \n",
    "    \n",
    "def extract_bow_features(documents):\n",
    "    \"\"\"\n",
    "    Perform bag of words feature extraction \n",
    "    \"\"\"\n",
    "    for doc, label in documents:\n",
    "        yield {\n",
    "            \"contains(\\\"{}\\\")\".format(token): True \n",
    "            for token in normalize(doc)\n",
    "        }, label\n",
    "    \n",
    "        \n",
    "        \n",
    "def extract_tfidf_features(documents):\n",
    "    \"\"\"\n",
    "    Perform TF-IDF feature extraction for a list of (document, label) pairs. \n",
    "    \"\"\"\n",
    "    # Separate the labels from the documents \n",
    "    labels    = [label for _, label in documents]\n",
    "    documents = [list(normalize(document)) for document, _ in documents]\n",
    "    \n",
    "    # Create the word index mapping \n",
    "    lexicon   = gensim.corpora.Dictionary(documents)\n",
    "\n",
    "    # Vectorize each document and create the TF-IDF model \n",
    "    documents = [lexicon.doc2bow(doc) for doc in documents]\n",
    "    tfidf     = gensim.models.TfidfModel(documents, normalize=True)\n",
    "    \n",
    "    # Note that you can save both the tfidf model and the lexicon to disk\n",
    "    # in order to load them later to featurize new documents. E.g. \n",
    "    # lexicon.save_as_text('~/models/baleen.lexicon')\n",
    "    # tfidf.save('~/models/baleen.tfidf_model')\n",
    "\n",
    "    for idx, vector in enumerate(documents):\n",
    "        # Compute the TF-IDF scores for the document as a map \n",
    "        dvec = dict(tfidf[vector])\n",
    "        \n",
    "        # Create the feature dictionary to use in an NLTK classifier\n",
    "        yield ({\n",
    "            \"tfidf(\\\"{}\\\")\".format(token):  dvec.get(tid, 0.0)\n",
    "            for tid, token in lexicon.items()\n",
    "        }, labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Featurize the corpus and create train test sets. \n",
    "documents = extract_bow_features(labeled_documents(corpus, categories=['design', 'books']))\n",
    "train, test = train_test_split(list(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Naive Bayes Classifier \n",
    "# The classifier expects a list of (feature, label) pairs \n",
    "# where the features are a dictionary of text features. \n",
    "classifier  = nltk.NaiveBayesClassifier.train(train)\n",
    "\n",
    "# Write the Naive Bayes Classifier to disk to use later \n",
    "with open(os.path.join(ROOT, 'fixtures', 'nbayes.pickle'), 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "# Show the accuracy of the classifier on the test set \n",
    "accuracy = nltk.classify.accuracy(classifier, test)\n",
    "print(\"Naive Bayes accuracy: {}\".format(accuracy))\n",
    "\n",
    "# Show the 30 most informative features \n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Maximum Entropy Classifier \n",
    "classifier = nltk.MaxentClassifier.train(train,\n",
    "    algorithm='megam', trace=2, gaussian_prior_sigma=1)\n",
    "\n",
    "# Write the Naive Bayes Classifier to disk to use later \n",
    "with open(os.path.join(ROOT, 'fixtures', 'maxent.pickle'), 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "# Show the accuracy of the classifier on the test set \n",
    "accuracy = nltk.classify.accuracy(classifier, test)\n",
    "print(\"Maximum Entropy accuracy: {}\".format(accuracy))\n",
    "\n",
    "# Show the 30 most informative features \n",
    "classifier.show_most_informative_features(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
