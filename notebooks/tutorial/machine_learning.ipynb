{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on Text/Language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk\n",
    "import math\n",
    "import gensim \n",
    "import pickle \n",
    "import random \n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module Variables\n",
    "ROOT   = os.getcwd() \n",
    "CORPUS = os.path.join(ROOT, \"fixtures\", \"tagged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Corpus Reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "class BaleenCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    Quick reader for the preprocessed tokenized and tagged version of the corpus. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, categoryids=CAT_PATTERN):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        CategorizedCorpusReader.__init__(self, {\"cat_pattern\": categoryids})\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "    \n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for token in sentence:\n",
    "                yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = BaleenCorpusReader(CORPUS)\n",
    "\n",
    "# Print statistics about each category. \n",
    "words = nltk.ConditionalFreqDist([\n",
    "        (category, word) \n",
    "        for category in corpus.categories()\n",
    "        for word in corpus.words(categories=category)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_science has 23,614 vocab and 261,494 words\n",
      "cinema has 45,956 vocab and 688,302 words\n",
      "business has 84,434 vocab and 2,258,435 words\n",
      "books has 44,302 vocab and 517,482 words\n",
      "cooking has 22,787 vocab and 258,994 words\n",
      "tech has 52,305 vocab and 870,464 words\n",
      "sports has 32,468 vocab and 579,188 words\n",
      "design has 21,972 vocab and 178,851 words\n",
      "do_it_yourself has 29,077 vocab and 322,049 words\n",
      "politics has 44,782 vocab and 1,031,593 words\n",
      "gaming has 37,759 vocab and 579,124 words\n",
      "news has 162,010 vocab and 8,441,547 words\n"
     ]
    }
   ],
   "source": [
    "for category, dist in words.items():\n",
    "    wc = sum(dist.values())\n",
    "    vb = len(dist) \n",
    "    print(\"{} has {:>,} vocab and {:>,} words\".format(category, vb, wc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers \n",
    "\n",
    "### Build Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(corpus, categories=None, test=0.2):\n",
    "    \"\"\"\n",
    "    Build a training and testing set of documents with their associated \n",
    "    labels by creating a list of documents for the specified categories, \n",
    "    shuffling them, then returning test% and 1-test% of the data set. \n",
    "    \n",
    "    Note: must specify a list of categories \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the total list of categories\n",
    "    categories = categories or corpus.categories() \n",
    "        \n",
    "    # Build a list of the documents with their associated words\n",
    "    # Note this loads the entire corpus into memory!\n",
    "    docs = [\n",
    "        (\n",
    "            list(corpus.words(fileids=fileid)), \n",
    "            corpus.categories(fileids=fileid)[0]   \n",
    "        )\n",
    "        for fileid in corpus.fileids(categories=categories)\n",
    "    ]\n",
    "    \n",
    "    # Shuffle the document in place \n",
    "    random.shuffle(docs) \n",
    "    \n",
    "    # Find the split index \n",
    "    split = math.floor(len(docs)*test)\n",
    "    \n",
    "    # Return the train/test based on the split \n",
    "    return docs[split:], docs[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(documents):\n",
    "    \"\"\"\n",
    "    Removes stopwords, lowercases, lemmatizes \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
