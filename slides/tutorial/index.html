<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Natural Language Processing with NLTK and Gensim: District Data Labs Tutorial at PyCon 2016</title>

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/ddl.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement( 'link' );
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
</head>

<body>
  <!-- Presentation containers -->
	<div class="reveal">
		<div class="slides">

    <!-- Introductory Slides -->
    <section>

      <!-- title slide -->
			<section id="title" data-markdown>
				<script type="text/template">
          ## Natural Language Processing with NLTK and Gensim
          May 29, 2016

          Slides, Notebooks, and Corpus can be found at [pycon.districtdatalabs.com](http://pycon.districtdatalabs.com)

          <aside class="notes">
            PyCon 2016 tutorial presented on Sunday, May 29 at 9am.
            Note that code in slides goes _down_!
          </aside>
				</script>
			</section><!-- title slide ends -->

      <!-- instructors slide -->
			<section id="instructor-ben" data-markdown>
				<script type="text/template">
					![Ben Word Cloud](images/ben_wordcloud.png)
          <aside class="notes">

          </aside>
        </script>
			</section>

			<section id="instructor-laura" data-markdown>
				<script type="text/template">

					![Laura Word Cloud](images/laura_wordcloud.png)
          <aside class="notes">

          </aside>
        </script>
			</section><!-- instructor slide ends -->

    </section>
    <!-- end introduction -->

    <!-- Part I: An Introduction to NLTK -->
    <section>

      <!-- part one title slide -->
      <section id="part-one" data-markdown>
				<script type="text/template">
          ## Introduction to NLTK and Gensim

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part one title slide ends -->

      <!-- nltk-nutshell slide -->
      <section id="nltk-nutshell" data-markdown>
        <script type="text/template">
          ### What is NLTK?

          - Python interface to over 50 corpora and lexical resources
          - Focus on machine learning applied to specific domains
          - Numpy and SciPy under the hood
          - Free and open source
          - Fast and formal
          - Batteries included

          <aside class="notes">

          </aside>
        </script>
      </section><!-- nltk-nutshell slide ends -->

      <!-- what-is-nltk slide -->
      <section id="what-is-nltk" data-markdown>
        <script type="text/template">
          ### What is NLTK?

          Suite of libraries for a variety of academic text processing tasks:

          - tokenization, stemming, tagging
          - chunking, parsing, classification
          - language modeling, logical semantics

          Pedagogical resources for teaching NLP theory in Python &hellip;

          <aside class="notes">

          </aside>
        </script>
      </section><!-- what-is-nltk slide ends -->

      <!-- nltk-authors slide -->
      <section id="nltk-authors" data-markdown>
        <script type="text/template">
          ### Who Wrote NLTK?

          <img src="images/nltk-authors.png" alt="NLTK Authors" style="background:none; border:none; box-shadow:none;"/>

          <aside class="notes">
            TODO: Add author images and bios as separate entities.
          </aside>
        </script>
      </section><!-- nltk-authors slide ends -->

      <!-- batteries-included slide -->
      <section id="batteries-included" data-markdown>
        <script type="text/template">
          ### Batteries Included

          <img src="images/batteries-included.png" alt="Batteries Included" style="background:none; border:none; box-shadow:none;"/>

          NLP = NLTK = Corpora + Algorithms

          Ready for Research!

          <aside class="notes">

          </aside>
        </script>
      </section><!-- batteries-included slide ends -->

      <!-- nltk-is-not slide -->
      <section id="nltk-is-not" data-markdown>
        <script type="text/template">
          ### What NLTK is not

          - NOT Production ready out of the box*
          - NOT Lightweight
          - NOT Generally applicable
          - NOT Magic

          <small><sup>*</sup>There are actually a few things that are production-ready right out of the box.</small>

          <aside class="notes">
             </aside>
        </script>
      </section><!-- nltk-is-not slide ends -->

      <!-- nltk-the-good slide -->
      <section id="nltk-the-good" data-markdown>
        <script type="text/template">
          ### The Good

          - Preprocessing
              - segmentation, tokenization, PoS tagging
          - Word level processing
              - WordNet, Lemmatization, Stemming, NGram
          - Utilities
              - Tree, FreqDist, ConditionalFreqDist
              - Streaming CorpusReader objects
          - Classification
              - Maximum Entropy, Naive Bayes, Decision Tree
              - Chunking, Named Entity Recognition
          - Parsers Galore!
          - Languages Galore!

          <aside class="notes">

          </aside>
        </script>
      </section><!-- nltk-the-good slide ends -->

      <!-- nltk-the-bad slide -->
      <section id="nltk-the-bad" data-markdown>
        <script type="text/template">
          ### The Bad

          - Syntactic Parsing
              - No included grammar (not a black box)
          - Feature/Dependency Parsing
              - No included feature grammar
          - The `sem` package
              - Toy only (lambda-calculus &amp; first order logic)
          - Lots of extra stuff
              - papers, chat programs, alignments, etc.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- nltk-the-bad slide ends -->

      <!-- other-libraries slide -->
      <section id="other-libraries" data-markdown>
        <script type="text/template">
          ### Other Python NLP Libraries

          - [TextBlob](http://textblob.readthedocs.org/en/dev/)
          - [SpaCy](https://spacy.io/)
          - [Scikit-Learn](http://scikit-learn.org/stable/)
          - [Pattern](http://www.clips.ua.ac.be/pattern)
          - [gensim](http://radimrehurek.com/gensim/)
          - [MITIE](https://github.com/mit-nlp/MITIE)
          - [guess_language](https://bitbucket.org/spirit/guess_language)
          - [Python wrapper for Stanford CoreNLP](https://github.com/brendano/stanford_corenlp_pywrapper)
          - [Python wrapper for Berkeley Parser](https://github.com/emcnany/berkeleyinterface)
          - [readability-lxml](https://pypi.python.org/pypi/readability-lxml)
          - [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)

          <aside class="notes">

          </aside>
        </script>
      </section><!-- other-libraries slide ends -->

      <!-- production-grade-nlp slide -->
      <section id="production-grade-nlp" data-markdown>
        <script type="text/template">
          ### Production Grade NLP

          <img src="images/production-grade-nlp.png" alt="Production Grade NLP" style="background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- production-grade-nlp slide ends -->

    </section>
    <!-- part one ends -->

    <!-- Part II: Working with Custom Corpora -->
    <section>

      <!-- part two title slide -->
      <section id="part-two" data-markdown>
				<script type="text/template">
          ## Working with Custom Corpora

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part two title slide ends -->

			<section data-background="images/ds-pipeline.png" data-background-size="800px">
			<h3>Data Science Pipeline</h3>
          <aside class="notes">
            TODO: Refactor images to stand on their own.
          </aside>
			</section><!-- end data-science-pipeline slide -->


      <!-- data-product-pipeline slide -->
      <section data-background="images/data_product_pipeline-pp.png" data-background-size="800px">
         <h3>Data Product Pipeline</h3>
					<aside class="notes">
            TODO:
          </aside>
      </section><!-- end data-product-pipeline slide -->

      <!-- baleen slide -->
      <section data-markdown>
				##Baleen

- There’s plenty of natural language all over the Internet that you can use to build a custom corpora IF ONLY YOU COULD GET IT
- One such implementation is Baleen, an RSS/Atom ingestion service that will regularly scrape the feeds specified in an OMPL format file for new content and store them as MongoDB documents for your later consumption

          <aside class="notes">
           
          </aside>
      </section><!-- end baleen slide -->

      <!-- start baleen architecture slide -->
<section data-background="images/baleen_service_architecture.png" data-background-size="800px">

</section>
      <!-- end baleen architecture slide -->

      <!-- baleen-dashboard slide -->
      <section>
          <h3>Baleen Dashboard</h3>

          Posts from Baleen Feeds are labelled by their feedtype category, which means we can use its data for supervised training of a model that can use text features to predict a Post’s category
          <br/>

          <img src="images/baleen-screenshot.png" height='300px'/>
          <aside class="notes">

          </aside>
      </section><!-- baleen-dashboard slide ends -->

      <!-- corpus-reader slide -->
      <section id="corpus-reader" data-markdown>
        <script type="text/template">
          ### CorpusReader Objects
- [CorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.api.CorpusReader)
- [SyntaxCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.api.SyntaxCorpusReader)
- [BNCCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bnc.BNCCorpusReader)
- [AlpinoCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bracket_parse.AlpinoCorpusReader)
- [BracketCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bracket_parse.BracketParseCorpusReader)
- [CategorizedSentencesCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.categorized_sents.CategorizedSentencesCorpusReader
)
- [ChasenCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.chasen.ChasenCorpusReader)
- [CHILDESCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.childes.CHILDESCorpusReader)
- [ChunkedCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.chunked.ChunkedCorpusReader)
- [ComparativeSentencesCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.comparative_sents.ComparativeSentencesCorpusReader)
- [ConllChunkCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.conll.ConllChunkCorpusReader)
....
and on and on
       
        </script>
      </section><!-- corpus-reader slide ends -->
<!-- lead in to corpus-reader notebook begins -->
<section data-markdown>
### Or subclass your own

```python
class BaleenCorpusReader(CategorizedCorpusReader, CorpusReader):
    """
    Quick reader for the preprocessed 
    tokenized and tagged version of the corpus. 
    """

    def __init__(self, root, fileids=PKL_PATTERN, 
    categoryids=CAT_PATTERN):
        # ...
```
    </section>
    <!-- part two ends -->
</section>
    <!-- Part III: Natural Language Processing -->
    <section>

      <!-- part three title slide -->
      <section id="part-three" data-markdown>
				<script type="text/template">
          ## Natural Language Processing

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part three title slide ends -->

      <!-- language slide -->
      <section id="language" data-markdown>
        <script type="text/template">
          <big>What is Language?</big><br />
          <small>Or, why should we analyze it?</small>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- langauge slide ends -->

      <!-- crab slide -->
      <section id="crab" data-markdown>
        <script type="text/template">
          <big>Crab</big>

          <small>What does this mean?</small>

          <aside class="notes">
            TODO: Bigger "Crab" symbol
          </aside>
        </script>
      </section><!-- crab slide ends -->

      <!-- crab-representation slide -->
      <section id="crab-representation" data-markdown>
        <script type="text/template">
          <img src="images/crab-representation.png" alt="Crab symbol" style="background:none; border:none; box-shadow:none;"/>

          <small>Is the symbol representative of its meaning?</small>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- crab-representation slide ends -->

      <!-- crab-mental-lexicon slide -->
      <section id="crab-mental-lexicon" data-markdown>
        <script type="text/template">
          <img src="images/crab-mental-lexicon.png" alt="Crab Mental Lexicon" style="background:none; border:none; box-shadow:none;"/>

          <small>Or is it just a mapping in a mental lexicon?</small>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- crab-mental-lexicon slide ends -->

      <!-- arbitrary-crab slide -->
      <section id="arbitrary-crab" data-markdown>
        <script type="text/template">
          <img src="images/arbitrary-crab.png" alt="Arbitrary Crab Sybmol" style="background:none; border:none; box-shadow:none;"/>

          <small>These symbols are arbitrary</small>

          <aside class="notes">
            TODO: Refactor Images on this slide
          </aside>
        </script>
      </section><!-- arbitrary-crab slide ends -->

      <!-- saussurian-linguistics slide -->
      <section id="saussurian-linguistics" data-markdown>
        <script type="text/template">
          ### What is Language?
          <img src="images/saussurian-linguistics.png" alt="Saussurian Linguistics" style="float: right; width: 40%; background:none; border:none; box-shadow:none;"/>
          #### Linguistic symbols:
          - Not acoustic "things"
          - Not mental processes

          #### Critical implications for:
          - Literature
          - Linguistics
          - Computer Science
          - Artificial Intelligence

          <aside class="notes">
            TODO: Refactor Images on this slide

            Ferdinand de Saussure was a Swiss linguist and semiotician whose ideas laid a foundation for many significant developments both in linguistics and semiology in the 20th century.
            discovery of the internal structure of the linguistic sign differentiated the sign both from mere acoustic 'things' ... and from mental processes, and that in this development new roads were thereby opened not only for linguistics, but also, in the future, for the theory of literature.
          </aside>
        </script>
      </section><!-- saussurian-linguistics slide ends -->

      <!-- evolution-of-linguistics slide -->
      <section id="evolution-of-linguistics" data-markdown>
        <script type="text/template">
          The science that has been developed around the facts of language passed through three
          stages before finding its true and unique object. First something called
          &ldquo;<em>grammar</em>&rdquo; was studied.

          This study, initiated by the Greeks and continued mainly by the French, was based on
          logic. It lacked a scientific approach and was detached from language itself.

          Its only aim was to give <em>rules</em> for distinguishing between correct and incorrect
          forms; it was a normative discipline, far removed from actual observation, and its
          scope was limited.

          &mdash; Ferdinand de Saussure

          <aside class="notes">

          </aside>
        </script>
      </section><!-- evolution-of-linguistics slide ends -->

      <!-- formal-vs-natural slide, fomal part -->
      <section id="formal-vs-natural-formal" data-markdown>

### Formal vs. Natural Languages


#### Formal Languages

- Strict, unchanging rules defined by grammars and parsed by regular expressions
- Generally application specific (chemistry, math)
- Literal: exactly what is said is meant
- No ambiguity
- Parsable by regular expressions
- Inflexible: no new terms or meaning

</section>


<section id="formal-vs-natural-natural" data-markdown>

### Formal vs. Natural Languages


#### Natural Languages

- Flexible, evolving language that occurs naturally in human communication
- Unspecific and used in many domains and applications
- Redundant and verbose in order to make up for ambiguity
- Expressive
- Difficult to parse
- Very flexible even in narrow contexts


          <aside class="notes">


          </aside>
     
      </section><!-- formal-vs-natural-natural slide ends -->

      <!-- cs-formal slide -->
      <section id="cs-formal" data-markdown>
        <script type="text/template">
          > Computer science has traditionally focused on formal languages.
          <aside class="notes">

          </aside>
      </script>
      </section><!-- cs-formal slide ends -->

      <!-- formal-language-parsing slide -->
      <section id="formal-language-parsing" data-markdown>
        <script type="text/template">
          <img src="images/formal-language-parsing.png" alt="Interpreter of Formal Language" style="background:none; border:none; box-shadow:none;"/>

          <small>Who has written a compiler or an interpreter?</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- formal-language-parsing slide ends -->

      <!-- hare-cut slide -->
      <section id="hare-cut" data-markdown>
        <script type="text/template">
          <img src="images/hare-cut.png" alt="Syntax Errors are your Friend!" style="background:none; border:none; box-shadow:none;"/>

          <small>Syntax Errors are your friend! [http://bbc.in/23pSPRq](http://bbc.in/23pSPRq)</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- hare-cut slide ends -->

      <!-- ambiguity slide -->
      <section id="ambiguity" data-markdown>
        <script type="text/template">
          > However, ambiguity is required for understanding when communicating
          > between people with diverse experience.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- ambiguity slide ends -->

      <!-- ml-for-nlp slide -->
      <section id="ml-for-nlp" data-markdown>
        <script type="text/template">
          > Natural Language Processing requires flexibility, which generally
          > comes from machine learning.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- ml-for-nlp slide ends -->

      <!-- language-models slide -->
      <section id="language-models" data-markdown>
        <script type="text/template">
          <img src="images/language-models.png" alt="Language Models" style="background:none; border:none; box-shadow:none;"/>

          <small>Language models are used for flexibility</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- language-models ends -->

      <!-- predictable-language slide -->
      <section id="predictable-language" data-markdown>
        <script type="text/template">
          ### Intuition: Language is Predictable (if flexible)

          &ldquo;There was a ton of traffic on the beltway so I was ________.&rdquo;

          &ldquo;At the beach we watched the ________.&rdquo;

          &ldquo;Watch out for that ________!&rdquo;

          <aside class="notes">

          </aside>
        </script>
      </section><!-- predictable-language ends -->

      <!-- token-relationships slide -->
      <section id="token-relationships" data-markdown>
        <script type="text/template">
          <img src="images/token-relationships.png" alt="Token Relationships" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <small>Language models predict relationships between tokens.</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- token-relationships slide ends -->

      <!-- symbol-meaning slide -->
      <section id="symbol-meaning" data-markdown>
        <script type="text/template">
          <img src="images/symbol-meaning.png" alt="Still not Meaning" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <small>But they can’t understand meaning (yet)</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- symbol-meaning slide ends -->

      <!-- tokens-not-words slide -->
      <section id="tokens-not-words" data-markdown>
        <script type="text/template">
          #### So please keep in mind:
          ## Tokens != Words
          <img src="images/tokens-not-words.png" alt="Tokens Not Words" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- tokens-not-words slide ends -->

      <!-- connectionist-vs-symbolic slide -->
      <section id="connectionist-vs-symbolic" data-markdown>
        <script type="text/template">
          ### Connectionist vs Symbolic Models
          <img src="images/connectionist-vs-symbolic.png" alt="Connectionist vs. Symbolic" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- connectionist-vs-symbolic slide ends -->

      <!-- state-of-the-art slide -->
      <section id="state-of-the-art" data-markdown>
        <script type="text/template">
          ### The State of the Art

          - Academic design for use alongside intelligent agents (AI discipline)
          - Relies on formal models or representations of knowledge &amp; language
          - Models are adapted and augmented through probabilistic methods and machine learning.
          - A small number of algorithms comprise the standard framework.
          <aside class="notes">

          </aside>
        </script>
      </section><!-- state-of-the-art slide ends -->

      <!-- traditional-nlp slide -->
      <section id="traditional-nlp" data-markdown>
        <script type="text/template">
          ### Traditional NLP Applications

          <style>
            ul.col {
              float:left;
            }
            ul.col li {
              width: 100%;
            }
          </style>

          <ul class="col">
            <li>Summarization</li>
            <li>Reference Resolution</li>
            <li>Machine Translation</li>
            <li>Language Generation</li>
            <li>Language Understanding</li>
            <li>Document Classification</li>
            <li>Author Identification</li>
            <li>Part of Speech Tagging</li>
          </ul>
          <ul class="col">
            <li>Question Answering</li>
            <li>Information Extraction</li>
            <li>Information Retrieval</li>
            <li>Speech Recognition</li>
            <li>Sense Disambiguation</li>
            <li>Topic Recognition</li>
            <li>Relationship Detection</li>
            <li>Named Entity Recognition</li>
          </ul>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- traditional-nlp slide ends -->

    </section>
    <!-- part three ends -->

    <!-- Part IV: Language Preprocessing with NLTK -->
    <section>

      <!-- part four title slide -->
      <section id="part-four" data-markdown>
				<script type="text/template">
          ## Language Preprocessing with NLTK

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part four title slide ends -->

      <!-- ingredients slide -->
      <section id="ingredients" data-markdown>
				<script type="text/template">
          ### What is Required?

          - Domain Knowledge
          - A Corpus in the Domain
          <img src="images/knowledge.png" alt="Knowledge" style="display: block; margin: 0 auto; max-height: 500px; border: none;"/>

          <aside class="notes">

          </aside>
				</script>
			</section><!-- ingredients slide ends -->

      <!-- nlp-pipeline slide -->
      <section id="nlp-pipeline" data-markdown>
				<script type="text/template">
          ### The NLP Pipeline

          <img src="images/nlp-pipeline.png" alt="The NLP Pipeline" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
				</script>
			</section><!-- nlp-pipeline slide ends -->

      <!-- discourse slide -->
      <section id="discourse" data-markdown>
        <script type="text/template">
          ### Discourse

          The study of the structure of language.

          Many documents have a set structure, some document formats (like HTML)
          directly expose the structure of the communication with tags.

          ####Major tasks

          - document parsing (e.g. HTML parsing), structural decomposition
          - document clean up, targeting

          <aside class="notes">

          </aside>
        </script>
      </section><!-- discourse slide ends -->

      <!-- morphology slide -->
      <section id="morphology" data-markdown>
				<script type="text/template">
          ### Morphology

          The study of the forms of things, words in particular.

          Consider pluralization for English:

          - Orthographic Rules: puppy → puppies
          - Morphological Rules: goose → geese or fish

          Major parsing tasks:

          - stemming, lemmatization and tokenization.
          <aside class="notes">

          </aside>
				</script>
			</section><!-- morphology slide ends -->

      <!-- syntax slide -->
      <section id="syntax" data-markdown>
				<script type="text/template">
          ### Syntax
          The study of the rules for the formation of sentences.

          Major tasks: chunking, parsing, feature parsing, grammars

          <img src="images/syntax.png" alt="Syntax" style="display: block; margin: 0 auto; max-height: 500px; border: none;"/>

          <aside class="notes">
            TODO: Convert image to SVG
          </aside>
				</script>
			</section><!-- syntax slide ends -->

      <!-- semantics slide -->
      <section id="semantics" data-markdown>
				<script type="text/template">
          ### Semantics

          <img src="images/mad-hatter.png" alt="Mad Hatter" style="float:right; display: block; margin: 0 auto; width:40%; border: none;"/>

          The study of meaning
          - I see what I eat.
          - I eat what I see.
          - He poached salmon.

          Major Tasks
          - Frame extraction
          - Creation of TMRs

          <aside class="notes">
            TODO: Convert image to SVG
          </aside>
				</script>
			</section><!-- semantics slide ends -->

      <!-- tmr slide -->
      <section id="tmr" data-markdown>
				<script type="text/template">
          ### Thematic Meaning Representations

          &ldquo;The man hit the building with the baseball bat&rdquo;

          ```python
          {
              "subject": {"text": "the man", "sense": human-agent},
              "predicate": {"text": "hit", "sense": strike-physical-force},
              "object": {"text": "the building", "sense": habitable-structure},
              "instrument": {"text": "with the baseball bat" "sense": sports-equipment}
          }
          ```

          <aside class="notes">

          </aside>
				</script>
			</section><!-- tmr slide ends -->

      <!-- recent-nlp-applications slide -->
      <section id="recent-nlp-applications" data-markdown>
				<script type="text/template">
          ### Recent NLP Applications

          - [Yelp Insights](http://officialblog.yelp.com/2012/04/yelpy-insights.html)
          - Winning Jeopardy! IBM Watson
          - Computer assisted medical coding ([3M Health Information Systems](http://solutions.3m.com/wps/portal/3M/en_US/Health-Information-Systems/HIS/Products-and-Services/Computer-Assisted-Coding/))
          - Geoparsing -- [CLAVIN](http://clavin.io/) (built by Charlie Greenbacker)
          - Author Identification (classification/clustering)
          - Sentiment Analysis (RTNNs, classification)
          - Language Detection
          - Event Detection
          - [Google Knowledge Graph](http://www.google.com/insidesearch/features/search/knowledge.html)
          - Named Entity Recognition and Classification
          - Machine Translation
          - Image + Language Processing

          <aside class="notes">

          </aside>
				</script>
			</section><!-- recent-nlp-applications slide ends -->

      <!-- big-data slide -->
      <section id="big-data" data-markdown>
				<script type="text/template">
          ### Applications are BIG data

          - Examples are easier to create than rules.
          - Rules and logic miss frequency and language dynamics.
          - More data is better for machine learning, relevance is in the long tail.
          - Knowledge engineering is not scalable.
          - Computational linguistics methodologies are stochastic.

          <aside class="notes">

          </aside>
				</script>
			</section><!-- big-data slide ends -->

      <!-- tokenization-tagging slide -->
      <section id="tokenization-tagging" data-markdown>
				<script type="text/template">
          ### Tokenization &amp; Tagging

          ```python
          text = u"Medical personnel returning to New York" #...

          for sent in nltk.sent_tokenize(text):
            print(sent)
          for sent in nltk.sent_tokenize(text):
            print(list(nltk.wordpunct_tokenize(sent)))
          for sent in nltk.sent_tokenize(text):
            print(list(nltk.pos_tag(nltk.word_tokenize(sent))))
          ```

          <aside class="notes">

          </aside>
				</script>
			</section><!-- tokenization-tagging slide ends -->

			<!-- stemming-lemmatization slide -->
      <section id="stemming-lemmatization" data-markdown>
				<script type="text/template">
					### Stemming &amp; Lemmatization

					Use stemming and lemmatization to normalize words across a corpora.
            </script>
        </section>

        <!-- stemming-lemmatization slide ends -->

        <!-- stemming-1 slide -->
        <section id="stemming-1" data-markdown>
          <script type="text/template">
### Stemming

Stemming = chop off affixes to get the root stem of the word:
* running &#8594; run
* flowers &#8594; flower
* geese   &#8594; geese
</script>
					<aside class="notes">

					</aside>

			</section><!-- stemming-1 slide ends -->

			<!-- stemming-2 slide -->
      <section id="stemming-2" data-markdown>
				<script type="text/template">
        <style>
            ul.col {
              float:left;
            }
            ul.col li {
              width: 100%;
            }
          </style>
          ### Stemming

          - Stemmers
            - Lancaster (English, newer and aggressive)
              - [class nltk.stem.lancaster.LancasterStemmer](http://www.nltk.org/_modules/nltk/stem/lancaster.html#LancasterStemmer)
            - Porter (English, original stemmer)
              - [class nltk.stem.porter.PorterStemmer](http://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer)
            - Snowball (Many languages, newest)
              - [class nltk.stem.snowball.EnglishStemmer](http://www.nltk.org/_modules/nltk/stem/snowball.html#EnglishStemmer)
              - And several others for other languages

          <aside class="notes">

          </aside>
				</script>
			</section><!-- stemming-2 slide ends -->

        <!-- stemming-code slide -->
      <section id="stemming-code" data-markdown>
        <script type="text/template">
### Stemming

```python
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem.porter import PorterStemmer

text = list(nltk.word_tokenize("The women running in the fog passed bunnies working as computer scientists."))

snowball = SnowballStemmer('english')
lancaster = LancasterStemmer()
porter = PorterStemmer()
```
</script>
          <aside class="notes">

          </aside>
    
      </section><!-- stemming-code slide ends -->

       <!-- lemmatization slide -->
      <section id="lemmatization" data-markdown>
        <script type="text/template">
          ### Lemmatization

          Lemmatization = look up word form in a lexicon to get canonical lemma:
          * women   &#8594; woman
          * foxes   &#8594; fox
          * sheep   &#8594; sheep
</script>
          <aside class="notes">

          </aside>
      
      </section><!-- lemmatization slide ends -->

      <!-- lemmatization-2 slide starts -->
      <section id="lemmaization-2" data-markdown>
        ## Lemmatization
        - Lemmatizer
            - WordNet lexicon
              - [class nltk.stem.wordnet.WordNetLemmatizer](http://www.nltk.org/_modules/nltk/stem/wordnet.html#WordNetLemmatizer)

            </section>
            <!-- lemmatization-2 slide ends -->

		

			<!-- lemmatization-code slide -->
			<section id="lemmatization-code" data-markdown>
				<script type="text/template">
          ### Lemmatization

          ```python
          from nltk.stem.wordnet import WordNetLemmatizer

          lemmatizer = WordNetLemmatizer()
          lemmas = [lemmatizer.lemmatize(t) for t in text]
          print(" ".join(lemmas))
          ```

          <aside class="notes">

					</aside>
				</script>
			</section><!-- lemmatization-code slide ends -->

			<!-- stemming-lemmatization-5 slide -->
			<section id="stemming-lemmatization-5" data-markdown>
				<script type="text/template">
          ### Stemming &amp; Lemmatization
          Typical normalization of text for use as features in machine learning models looks something like this:

          ```python
          import string
          ## Module constants
          lemmatizer  = WordNetLemmatizer()
          stopwords   = set(nltk.corpus.stopwords.words('english'))
          punctuation = string.punctuation

          def normalize(text):
            for token in nltk.word_tokenize(text):
              token = token.lower()
              token = lemmatizer.lemmatize(token)
              if token not in stopwords and token not in punctuation:
                yield token

          print(list(normalize("The eagle flies at midnight.")))
          ```

          <aside class="notes">

					</aside>
				</script>
			</section><!-- stemming-lemmatization-5 slide ends -->

			<!-- parsing slide -->
			<section id="parsing" data-markdown>
				<script type="text/template">
          ### Parsing

          ```python
          import nltk

          grammar = nltk.grammar.CFG.fromstring("""
          S -> NP
          NP -> N N | ADJP NP | DET N
          ADJP -> ADJ NP
          DET -> 'an'
          N -> 'airplane'
          """)

          parser = nltk.parse.ChartParser(grammar)
          p = list(parser.parse(nltk.word_tokenize("an airplane")))

          for a in p:
            a.pprint()
            # p[0].draw()
          ```

          <aside class="notes">

					</aside>
				</script>
			</section><!-- parsing slide ends -->

			<!-- named-entity-recognition slide -->
			<section id="named-entity-recognition" data-markdown>
				<script type="text/template">
          ### Named Entity Recognition

          <aside class="notes">

					</aside>
				</script>
			</section><!-- named-entity-recognition slide ends -->

    </section>
    <!-- part four ends -->

    <!-- Part V: Textual Machine Learning -->
    <section>

      <!-- part five title slide -->
      <section id="part-five" data-markdown>
				<script type="text/template">
          ## Textual Machine Learning

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part five title slide ends -->

      <!-- instances slide -->
      <section id="instances" data-markdown>
        <script type="text/template">
          Machine learning uses <em>instances</em> (examples) of data to fit a parameterized model which is used to make predictions concerning new instances.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- instances slide ends -->

      <!-- what-are-instances slide -->
      <section id="what-are-instances" data-markdown>
        <script type="text/template">
          In text analysis, what are the instances?

          <aside class="notes">

          </aside>
        </script>
      </section><!-- what-are-instances slide ends -->

      <!-- instances-are-documents slide -->
      <section id="instances-are-documents" data-markdown>
        <script type="text/template">
          Instances = Documents

          (no matter their size)

          <img src="images/book.png" alt="Book" style="width: 25%; border: none;"/>
          <img src="images/twitter.png" alt="Twitter" style="width: 25%; border: none;"/>
          <img src="images/gmail.png" alt="Gmail" style="width: 25%; border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- instances-are-documents slide ends -->

      <!-- document-collection slide -->
      <section id="document-collection" data-markdown>
        <script type="text/template">
          <img src="images/bookcollection.png" alt="Book Collection" style="border: none;"/>

          A corpus is a collection of documents to learn about.

          (labeled or unlabeled)

          <aside class="notes">

          </aside>
        </script>
      </section><!-- document-collection slide ends -->

      <!-- features slide -->
      <section id="features" data-markdown>
        <script type="text/template">
          Features describe instances in a way that machines can learn on by putting them into feature space.

          <img src="images/feature-space.png" alt="Feature Space" style="width: 25%; border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- features slide ends -->

      <!-- document-features slide -->
      <section id="document-features" data-markdown>
        <script type="text/template">
          ### Document Features

          <img src="images/document-features.png" alt="Document Features" style="float:right; border: none; width: 20%;"/>

          #### Document level features
          - Metadata: title, author
          - Paragraphs
          - Sentence construction

          #### Word level features
          - Vocabulary
          - Form (capitalization)
          - Frequency

          <aside class="notes">

          </aside>
        </script>
      </section><!-- document-features slide ends -->

      <!-- vector-encoding slide -->
      <section id="vector-encoding" data-markdown>
        <script type="text/template">
          ### Vector Encoding

          - Basic representation of documents: a vector whose length is equal to the vocabulary of the entire corpus.
          - Word positions in the vector are based on lexicographic order.

          <img src="images/vector-encoding.png" alt="Vector Encoding" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- vector-encoding slide ends -->

      <!-- token-frequency slide -->
      <section id="token-frequency" data-markdown>
        <script type="text/template">
          ### Bag of Words: Token Frequency

          - One of the simplest models: compute the frequency of words in the document and use those numbers as the vector encoding.

          <img src="images/token-frequency.png" alt="Token Frequency" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- token-frequency slide ends -->

      <!-- one-hot-encoding slide -->
      <section id="one-hot-encoding" data-markdown>
        <script type="text/template">
          ### One Hot Encoding

          - The feature vector encodes the vocabulary of the document.
          - All words are equally distant, so must reduce word forms.
          - Usually used for artificial neural network models.

          <img src="images/one-hot-encoding.png" alt="One Hot Encoding" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- one-hot-encoding slide ends -->

      <!-- tf-idf-encoding slide -->
      <section id="tf-idf-encoding" data-markdown>
        <script type="text/template">
          ### TF-IDF Encoding

          - Highlight terms that are very relevant to a document relative to the rest of the corpus by computing the term frequency times the inverse document frequency of the term.

          <img src="images/tf-idf-encoding.png" alt="TF IDF Encoding" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- tf-idf-encoding slide ends -->

      <!-- distributed-representation slide -->
      <section id="distributed-representation" data-markdown>
        <script type="text/template">
          ### Distributed Representation

          - Dense representations along a continuous scale that can be used to encode similarity within vector space.

          <img src="images/distributed-representation.png" alt="Distributed Representation" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- distributed-representation slide ends -->

      <!-- distributed-representation-example slide -->
      <section id="distributed-representation-example" data-markdown>
        <script type="text/template">
          ### Distributed Representation with gensim.models.Doc2Vec

          ```python
          >>> import gensim
          >>> documents = gensim.models.doc2vec.TaggedLineDocument("documents.txt")
          >>> model = gensim.models.doc2vec.Doc2Vec(documents, size=7, min_count=0)
          >>> model.docvecs[0]

          array([-0.0225403 , -0.0212964 ,  0.02708783,  0.0049877 ,  0.04926294,
                 -0.03268785, -0.03209411], dtype=float32)
          ```

          <aside class="notes">

          </aside>
        </script>
      </section><!-- distributed-representation-example slide ends -->

      <!-- vector-encoding-pros-and-cons slide -->
      <section id="vector-encoding-pros-and-cons" data-markdown>
        <script type="text/template">
          ### Pros and Cons of Vector Encoding

          #### Pros
          - Machine learning requires a vector anyway.
          - Can embed complex representations like TF-IDF into the vector form.
          - Drives towards token-concept mapping without rules.

          #### Cons
          - The vectors have lots of columns (high dimension)
          - Word order, grammar, and other structural features are natively lost.
          - Difficult to add knowledge to learning process.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- vector-encoding-pros-and-cons slide ends -->

      <!-- much-work slide -->
      <section id="much-work" data-markdown>
        <script type="text/template">
          In the end, much of the work for language aware applications comes from domain specific feature analysis; not just simple vectorization.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- much-work slide ends -->

     

     

     

      <!-- designing-the-problem slide-->
      <section id="designing-the-problem" data-markdown>
        <script type="text/template">
          Classification and clustering are the two fundamental techniques that are the basis of most NLP.

          It’s all about designing the problem correctly.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- designing-the-problem slide ends -->

    </section>
    <!-- part five ends -->

    <!-- Part VI: Building Text Classifiers -->
    <section>

      <!-- part six title slide -->
      <section id="part-six" data-markdown>
				<script type="text/template">
          ## Building Text Classifiers

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part six title slide ends -->
 
  <!-- classification slide -->
      <section id="classification" data-markdown>
        <script type="text/template">
          ### Classification

          - Classification
            - Supervised ML
            - Requires pre-labeled corpus of documents
            - Sentiment Analysis
            - Models:
              - Naive Bayes
              - Maximum Entropy

          <aside class="notes">

          </aside>
        </script>
      </section><!-- classification slide ends -->


 <!-- classification-pipeline slide-->
      <section id="classification-pipeline" data-markdown>
        <script type="text/template">
          ### Classification Pipeline

          <img src="images/classification-pipeline.png" alt="Classification Pipeline" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- classification-pipeline slide ends -->
    </section>
    <!-- part six ends -->

    <!-- Part VII: Building Topic Models -->
    <section>

      <!-- part seven title slide -->
      <section id="part-seven" data-markdown>
				<script type="text/template">
          ## Building Topic Models

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part seven title slide ends -->


      <!-- clustering slide -->
      <section id="clustering" data-markdown>
        <script type="text/template">
          ### Clustering

          - Clustering
            - Unsupervised ML
            - Groups similar documents together
            - Topic Modeling
            - Models:
              - K means
              - LDA

          <aside class="notes">

          </aside>
        </script>
      </section><!-- clustering slide ends -->

       <!-- topic-modeling-pipeline slide-->
      <section id="topic-modeling-pipeline" data-markdown>
        <script type="text/template">
          ### Topic Modeling Pipeline

          <img src="images/classification-pipeline.png" alt="Classification Pipeline" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- topic-modeling-pipeline slide ends -->

    </section>
    <!-- part seven ends -->

    <!-- Concluding Remarks -->
    <section>

      <!-- conclusion slide -->
      <section id="conclusion" data-markdown>
				<script type="text/template">
          ## Concluding Remarks

          <aside class="notes">

          </aside>
				</script>
			</section><!-- conclusion slide ends -->

      <!-- sprints slide -->
      <section id="sprints" data-markdown>
				<script type="text/template">
          ## Sprints

          <aside class="notes">

          </aside>
				</script>
			</section><!-- sprints slide ends -->

      <!-- questions slide -->
      <section id="questions" data-markdown>
				<script type="text/template">
          ## Questions?

          <aside class="notes">

          </aside>
				</script>
			</section><!-- questions slide ends -->

    </section>
    <!-- concluding remarks ends -->


		</div><!-- slides ends -->
	</div><!-- reveal ends -->

	<script src="lib/js/head.min.js"></script>
	<script src="js/reveal.js"></script>

	<script>
		// More info https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			history: true,

			// More info https://github.com/hakimel/reveal.js#dependencies
			dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
			]
		});
	</script>
</body>
</html>
