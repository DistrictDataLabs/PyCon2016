<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Natural Language Processing with NLTK and Gensim: District Data Labs Tutorial at PyCon 2016</title>

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/ddl.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement( 'link' );
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
</head>

<body>
  <!-- Presentation containers -->
	<div class="reveal">
		<div class="slides">

    <!-- Introductory Slides -->
    <section>

      <!-- title slide -->
			<section id="title" data-markdown>
				<script type="text/template">
          ## Natural Language Processing with NLTK and Gensim
          May 29, 2016

          Slides, Notebooks, and Corpus can be found at [pycon.districtdatalabs.com](http://pycon.districtdatalabs.com)

          <aside class="notes">
            PyCon 2016 tutorial presented on Sunday, May 29 at 9am.
            Note that code in slides goes _down_!
          </aside>
				</script>
			</section><!-- title slide ends -->

      <!-- instructors slide -->
			<section id="instructor-ben" data-markdown>
				<script type="text/template">
					![Ben Word Cloud](images/ben_wordcloud.png)
          <aside class="notes">

          </aside>
        </script>
			</section>

			<section id="instructor-laura" data-markdown>
				<script type="text/template">

					![Laura Word Cloud](images/laura_wordcloud.png)
          <aside class="notes">

          </aside>
        </script>
			</section><!-- instructor slide ends -->

    </section>
    <!-- end introduction -->

    <!-- Part I: An Introduction to NLTK -->
    <section>

      <!-- part one title slide -->
      <section id="part-one" data-markdown>
				<script type="text/template">
          ## Introduction to NLTK and Gensim

          <aside class="notes">
          <a href="#common_contexts">common_contexts</a>
          <a href="#concordance">concordance</a>
          <a href="#collocations">collocations</a>
          <a href="#count">count</a>
          <a href="#plot">plot</a>
          <a href='#findall'>findall</a>
          <a href='#index'>index</a>
          <h2 id="common_contexts">common_contexts</h2>
          <h2 id='concordance'>concordance</h2>
          <p><a href="http://www.nltk.org/api/nltk.html#nltk.text.Text.concordance">DOCS</a>
          <p>Print a concordance for word with the specified context window. Word matching is not case-sensitive.</p>
          <h2 id='collocations'>collocations</h2>
          <p><a href="http://www.nltk.org/api/nltk.html#nltk.text.Text.collocations">DOCS</a></p>
          <p>Print collocations derived from the text, ignoring stopwords. Collocations are words that often appear consecutively within corpora.</p>
         <!-- <h2 id='count'>count</h2> -->
          <h2 id='plot'>plot</h2>
          <p><a href="http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist.plot">DOCS</a></p>
          <p>Plot samples from the frequency distribution displaying the most frequent sample first. If an integer parameter is supplied, stop after this many samples have been plotted. For a cumulative plot, specify cumulative=True. (Requires Matplotlib to be installed.)</p>
          <h2 id='findall'>findall</h2>
          <p><a href="http://www.nltk.org/api/nltk.html#nltk.text.TokenSearcher.findall">DOCS</a></p>
          <p>Find instances of the regular expression in the text. The text is a list of tokens, and a regexp pattern to match a single token must be surrounded by angle brackets.</p>
          <h2 id='index'>index</h2>
          <p><a href="http://www.nltk.org/api/nltk.html#nltk.downloader.Downloader.index">DOCS</a></p>
          <p>Return the XML index describing the packages available from the data server. If necessary, this index will be downloaded from the data server.</p>
          <p><a href="http://www.nltk.org/api/nltk.html#nltk.text.ContextIndex.common_contexts">DOCS</a>
          <p>Find contexts where the specified words can all appear; and return a frequency distribution mapping each context to the number of times that context was used.</p>
          </aside>
				</script>
			</section><!-- part one title slide ends -->

      <!-- nltk-nutshell slide -->
      <section id="nltk-nutshell" data-markdown>
        <script type="text/template">
          ### What is NLTK?

          - Python interface to over 50 corpora and lexical resources
          - Focus on machine learning applied to specific domains
          - Numpy and SciPy under the hood
          - Free and open source
          - Fast and formal
          - Batteries included

          <aside class="notes">

          </aside>
        </script>
      </section><!-- nltk-nutshell slide ends -->

      <!-- what-is-nltk slide -->
      <section id="what-is-nltk" data-markdown>
        <script type="text/template">
          ### What is NLTK?

          Suite of libraries for a variety of academic text processing tasks:

          - tokenization, stemming, tagging
          - chunking, parsing, classification
          - language modeling, logical semantics

          Pedagogical resources for teaching NLP theory in Python &hellip;

          <aside class="notes">

          </aside>
        </script>
      </section><!-- what-is-nltk slide ends -->

      <!-- nltk-authors slide -->
      <section id="nltk-authors" data-markdown>
        <script type="text/template">
          ### Who Wrote NLTK?

          <img src="images/nltk-authors.png" alt="NLTK Authors" style="background:none; border:none; box-shadow:none;"/>

          <aside class="notes">
        
          </aside>
        </script>
      </section><!-- nltk-authors slide ends -->

      <!-- batteries-included slide -->
      <section id="batteries-included" data-markdown>
        <script type="text/template">
          ### Batteries Included

          <img src="images/batteries-included.png" alt="Batteries Included" style="background:none; border:none; box-shadow:none;"/>

          NLP = NLTK = Corpora + Algorithms

          Ready for Research!

          <aside class="notes">

          </aside>
        </script>
      </section><!-- batteries-included slide ends -->

      <!-- nltk-is-not slide -->
      <section id="nltk-is-not" data-markdown>
        <script type="text/template">
          ### What NLTK is not

          - NOT Production ready out of the box*
          - NOT Lightweight
          - NOT Generally applicable
          - NOT Magic

          <small><sup>*</sup>There are actually a few things that are production-ready right out of the box.</small>

          <aside class="notes">
             </aside>
        </script>
      </section><!-- nltk-is-not slide ends -->

      <!-- nltk-the-good slide -->
      <section id="nltk-the-good" data-markdown>
        <script type="text/template">
          ### The Good

          - Preprocessing
              - segmentation, tokenization, PoS tagging
          - Word level processing
              - WordNet, Lemmatization, Stemming, NGram
          - Utilities
              - Tree, FreqDist, ConditionalFreqDist
              - Streaming CorpusReader objects
          - Classification
              - Maximum Entropy, Naive Bayes, Decision Tree
              - Chunking, Named Entity Recognition
          - Parsers Galore!
          - Languages Galore!

          <aside class="notes">

          </aside>
        </script>
      </section><!-- nltk-the-good slide ends -->

      <!-- nltk-the-bad slide -->
      <section id="nltk-the-bad" data-markdown>
        <script type="text/template">
          ### The Bad

          - Syntactic Parsing
              - No included grammar (not a black box)
          - Feature/Dependency Parsing
              - No included feature grammar
          - The `sem` package
              - Toy only (lambda-calculus &amp; first order logic)
          - Lots of extra stuff
              - papers, chat programs, alignments, etc.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- nltk-the-bad slide ends -->

      <!-- other-libraries slide -->
      <section id="other-libraries" data-markdown>
        <script type="text/template">
          ### Other Python NLP Libraries

          - [TextBlob](http://textblob.readthedocs.org/en/dev/)
          - [SpaCy](https://spacy.io/)
          - [Scikit-Learn](http://scikit-learn.org/stable/)
          - [Pattern](http://www.clips.ua.ac.be/pattern)
          - [gensim](http://radimrehurek.com/gensim/)
          - [MITIE](https://github.com/mit-nlp/MITIE)
          - [guess_language](https://bitbucket.org/spirit/guess_language)
          - [Python wrapper for Stanford CoreNLP](https://github.com/brendano/stanford_corenlp_pywrapper)
          - [Python wrapper for Berkeley Parser](https://github.com/emcnany/berkeleyinterface)
          - [readability-lxml](https://pypi.python.org/pypi/readability-lxml)
          - [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)

          <aside class="notes">

          </aside>
        </script>
      </section><!-- other-libraries slide ends -->

      <!-- production-grade-nlp slide -->
      <section id="production-grade-nlp" data-markdown>
        <script type="text/template">
          ### Production Grade NLP

          <img src="images/production-grade-nlp.png" alt="Production Grade NLP" style="background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- production-grade-nlp slide ends -->

    </section>
    <!-- part one ends -->

    <!-- Part II: Working with Custom Corpora -->
    <section>

      <!-- part two title slide -->
      <section id="part-two" data-markdown>
				<script type="text/template">
          ## Working with Custom Corpora

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part two title slide ends -->

			<section data-background="images/ds-pipeline.png" data-background-size="800px">
			<h3>Data Science Pipeline</h3>
          <aside class="notes">
            TODO: Refactor images to stand on their own.
          </aside>
			</section><!-- end data-science-pipeline slide -->


      <!-- data-product-pipeline slide -->
      <section data-background="images/data_product_pipeline-pp.png" data-background-size="800px">
         <h3>Data Product Pipeline</h3>
					<aside class="notes">
            TODO:
          </aside>
      </section><!-- end data-product-pipeline slide -->

      <!-- baleen slide -->
      <section data-markdown>
				##Baleen

- There’s plenty of natural language all over the Internet that you can use to build a custom corpora IF ONLY YOU COULD GET IT
- One such implementation is Baleen, an RSS/Atom ingestion service that will regularly scrape the feeds specified in an OMPL format file for new content and store them as MongoDB documents for your later consumption

          <aside class="notes">
           
          </aside>
      </section><!-- end baleen slide -->

      <!-- start baleen architecture slide -->
<section data-background="images/baleen_service_architecture.png" data-background-size="800px">

</section>
      <!-- end baleen architecture slide -->

      <!-- baleen-dashboard slide -->
      <section>
          <h3>Baleen Dashboard</h3>

          Posts from Baleen Feeds are labelled by their feedtype category, which means we can use its data for supervised training of a model that can use text features to predict a Post’s category
          <br/>

          <img src="images/baleen-screenshot.png" height='300px'/>
          <aside class="notes">

          </aside>
      </section><!-- baleen-dashboard slide ends -->

      <!-- corpus-reader slide -->
      <section id="corpus-reader" data-markdown>
        <script type="text/template">
          ### CorpusReader Objects
- [CorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.api.CorpusReader)
- [SyntaxCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.api.SyntaxCorpusReader)
- [BNCCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bnc.BNCCorpusReader)
- [AlpinoCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bracket_parse.AlpinoCorpusReader)
- [BracketCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bracket_parse.BracketParseCorpusReader)
- [CategorizedSentencesCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.categorized_sents.CategorizedSentencesCorpusReader
)
- [ChasenCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.chasen.ChasenCorpusReader)
- [CHILDESCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.childes.CHILDESCorpusReader)
- [ChunkedCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.chunked.ChunkedCorpusReader)
- [ComparativeSentencesCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.comparative_sents.ComparativeSentencesCorpusReader)
- [ConllChunkCorpusReader](http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.conll.ConllChunkCorpusReader)
....
and on and on
       
        </script>
      </section><!-- corpus-reader slide ends -->
<!-- lead in to corpus-reader notebook begins -->
<section data-markdown>
### Or subclass your own

```python
class BaleenCorpusReader(CategorizedCorpusReader, CorpusReader):
    """
    Quick reader for the preprocessed 
    tokenized and tagged version of the corpus. 
    """

    def __init__(self, root, fileids=PKL_PATTERN, 
    categoryids=CAT_PATTERN):
        # ...
```
    </section>
    <!-- part two ends -->
</section>
    <!-- Part III: Natural Language Processing -->
    <section>

      <!-- part three title slide -->
      <section id="part-three" data-markdown>
				<script type="text/template">
          ## Natural Language Processing

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part three title slide ends -->

      <!-- language slide -->
      <section id="language" data-markdown>
        <script type="text/template">
          <big>What is Language?</big><br />
          <small>Or, why should we analyze it?</small>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- langauge slide ends -->

      <!-- crab slide -->
      <section id="crab" data-markdown>
        <script type="text/template">
          <big>Crab</big>

          <small>What does this mean?</small>

          <aside class="notes">
            TODO: Bigger "Crab" symbol
          </aside>
        </script>
      </section><!-- crab slide ends -->

      <!-- crab-representation slide -->
      <section id="crab-representation" data-markdown>
        <script type="text/template">
          <img src="images/crab-representation.png" alt="Crab symbol" style="background:none; border:none; box-shadow:none;"/>

          <small>Is the symbol representative of its meaning?</small>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- crab-representation slide ends -->

      <!-- crab-mental-lexicon slide -->
      <section id="crab-mental-lexicon" data-markdown>
        <script type="text/template">
          <img src="images/crab-mental-lexicon.png" alt="Crab Mental Lexicon" style="background:none; border:none; box-shadow:none;"/>

          <small>Or is it just a mapping in a mental lexicon?</small>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- crab-mental-lexicon slide ends -->

      <!-- arbitrary-crab slide -->
      <section id="arbitrary-crab" data-markdown>
        <script type="text/template">
          <img src="images/arbitrary-crab.png" alt="Arbitrary Crab Sybmol" style="background:none; border:none; box-shadow:none;"/>

          <small>These symbols are arbitrary</small>

          <aside class="notes">
            TODO: Refactor Images on this slide
          </aside>
        </script>
      </section><!-- arbitrary-crab slide ends -->

      <!-- saussurian-linguistics slide -->
      <section id="saussurian-linguistics" data-markdown>
        <script type="text/template">
          ### What is Language?
          <img src="images/saussurian-linguistics.png" alt="Saussurian Linguistics" style="float: right; width: 40%; background:none; border:none; box-shadow:none;"/>
          #### Linguistic symbols:
          - Not acoustic "things"
          - Not mental processes

          #### Critical implications for:
          - Literature
          - Linguistics
          - Computer Science
          - Artificial Intelligence

          <aside class="notes">
            <a href="ferdinand">Ferdinand de Saussure</a>

            <h2 id="ferdinand">Ferdinand de Saussure</h2>
            Ferdinand de Saussure was a Swiss linguist and semiotician whose ideas laid a foundation for many significant developments both in linguistics and semiology in the 20th century.
            discovery of the internal structure of the linguistic sign differentiated the sign both from mere acoustic 'things' ... and from mental processes, and that in this development new roads were thereby opened not only for linguistics, but also, in the future, for the theory of literature.

            <h2 id="semiotics">Semiotics</h2>
            Semiotics (also called semiotic studies; not to be confused with the Saussurean tradition called semiology which is a part of semiotics) is the study of meaning-making, the study of sign processes and meaningful communication.[1] This includes the study of signs and sign processes (semiosis), indication, designation, likeness, analogy, metaphor, symbolism, signification, and communication.

<p>Semiotics is closely related to the field of linguistics, which, for its part, studies the structure and meaning of language more specifically. The semiotic tradition explores the study of signs and symbols as a significant part of communications. As different from linguistics, however, semiotics also studies non-linguistic sign systems. Semiotics may be divided into three branches:</p>

    <ul>
    <li>Semantics: relation between signs and the things to which they refer; their signified denotata, or meaning</li>
    <li>Syntactics: relations among or between signs in formal structures</li>
    <li>Pragmatics: relation between signs and sign-using agents or interpreters</li></ul>

<p>Semiotics is frequently seen as having important anthropological dimensions; for example, the late Italian semiotician and novelist Umberto Eco proposed that every cultural phenomenon may be studied as communication.[2] Some semioticians focus on the logical dimensions of the science, however. They examine areas belonging also to the life sciences—such as how organisms make predictions about, and adapt to, their semiotic niche in the world (see semiosis). In general, semiotic theories take signs or sign systems as their object of study: the communication of information in living organisms is covered in biosemiotics (including zoosemiotics).</p>

<p>Syntactics is the branch of semiotics that deals with the formal properties of signs and symbols.[3] More precisely, syntactics deals with the "rules that govern how words are combined to form phrases and sentences". Charles Morris adds that semantics deals with the relation of signs to their designata and the objects that they may or do denote; and, pragmatics deals with the biotic aspects of semiosis, that is, with all the psychological, biological, and sociological phenomena that occur in the functioning of signs.(<a href="https://en.wikipedia.org/wiki/Semiotics"></p>Wikipedia</a>)
          </aside>
        </script>
      </section><!-- saussurian-linguistics slide ends -->

      <!-- evolution-of-linguistics slide -->
      <section id="evolution-of-linguistics" data-markdown>
        <script type="text/template">
          The science that has been developed around the facts of language passed through three
          stages before finding its true and unique object. First something called
          &ldquo;<em>grammar</em>&rdquo; was studied.

          This study, initiated by the Greeks and continued mainly by the French, was based on
          logic. It lacked a scientific approach and was detached from language itself.

          Its only aim was to give <em>rules</em> for distinguishing between correct and incorrect
          forms; it was a normative discipline, far removed from actual observation, and its
          scope was limited.

          &mdash; Ferdinand de Saussure

          <aside class="notes">

          </aside>
        </script>
      </section><!-- evolution-of-linguistics slide ends -->

      <!-- formal-vs-natural slide, fomal part -->
      <section id="formal-vs-natural-formal" data-markdown>

### Formal vs. Natural Languages


#### Formal Languages

- Strict, unchanging rules defined by grammars and parsed by regular expressions
- Generally application specific (chemistry, math)
- Literal: exactly what is said is meant
- No ambiguity
- Parsable by regular expressions
- Inflexible: no new terms or meaning

</section>


<section id="formal-vs-natural-natural" data-markdown>

### Formal vs. Natural Languages


#### Natural Languages

- Flexible, evolving language that occurs naturally in human communication
- Unspecific and used in many domains and applications
- Redundant and verbose in order to make up for ambiguity
- Expressive
- Difficult to parse
- Very flexible even in narrow contexts


          <aside class="notes">


          </aside>
     
      </section><!-- formal-vs-natural-natural slide ends -->

      <!-- cs-formal slide -->
      <section id="cs-formal" data-markdown>
        <script type="text/template">
          > Computer science has traditionally focused on formal languages.
          <aside class="notes">

          </aside>
      </script>
      </section><!-- cs-formal slide ends -->

      <!-- formal-language-parsing slide -->
      <section id="formal-language-parsing" data-markdown>
        <script type="text/template">
          <img src="images/formal-language-parsing.png" alt="Interpreter of Formal Language" style="background:none; border:none; box-shadow:none;"/>

          <small>Who has written a compiler or an interpreter?</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- formal-language-parsing slide ends -->

      <!-- hare-cut slide -->
      <section id="hare-cut" data-markdown>
        <script type="text/template">
          <img src="images/hare-cut.png" alt="Syntax Errors are your Friend!" style="background:none; border:none; box-shadow:none;"/>

          <small>Syntax Errors are your friend! [http://bbc.in/23pSPRq](http://bbc.in/23pSPRq)</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- hare-cut slide ends -->

      <!-- ambiguity slide -->
      <section id="ambiguity" data-markdown>
        <script type="text/template">
          > However, ambiguity is required for understanding when communicating
          > between people with diverse experience.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- ambiguity slide ends -->

      <!-- ml-for-nlp slide -->
      <section id="ml-for-nlp" data-markdown>
        <script type="text/template">
          > Natural Language Processing requires flexibility, which generally
          > comes from machine learning.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- ml-for-nlp slide ends -->

      <!-- language-models slide -->
      <section id="language-models" data-markdown>
        <script type="text/template">
          <img src="images/language-models.png" alt="Language Models" style="background:none; border:none; box-shadow:none;"/>

          <small>Language models are used for flexibility</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- language-models ends -->

      <!-- predictable-language slide -->
      <section id="predictable-language" data-markdown>
        <script type="text/template">
          ### Intuition: Language is Predictable (if flexible)

          &ldquo;There was a ton of traffic on the beltway so I was ________.&rdquo;

          &ldquo;At the beach we watched the ________.&rdquo;

          &ldquo;Watch out for that ________!&rdquo;

          <aside class="notes">

          </aside>
        </script>
      </section><!-- predictable-language ends -->

      <!-- token-relationships slide -->
      <section id="token-relationships" data-markdown>
        <script type="text/template">
          <img src="images/token-relationships.png" alt="Token Relationships" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <small>Language models predict relationships between tokens.</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- token-relationships slide ends -->

      <!-- symbol-meaning slide -->
      <section id="symbol-meaning" data-markdown>
        <script type="text/template">
          <img src="images/symbol-meaning.png" alt="Still not Meaning" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <small>But they can’t understand meaning (yet)</small>
          <aside class="notes">

          </aside>
        </script>
      </section><!-- symbol-meaning slide ends -->

      <!-- tokens-not-words slide -->
      <section id="tokens-not-words" data-markdown>
        <script type="text/template">
          #### So please keep in mind:
          ## Tokens != Words
          <img src="images/tokens-not-words.png" alt="Tokens Not Words" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- tokens-not-words slide ends -->

      <!-- connectionist-vs-symbolic slide -->
      <section id="connectionist-vs-symbolic" data-markdown>
        <script type="text/template">
          ### Connectionist vs Symbolic Models
          <img src="images/connectionist-vs-symbolic.png" alt="Connectionist vs. Symbolic" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          <a href="#tensornetwork">Tensor Networks</a>
          <a href="#nnmf">NNMF</a>
          <a href="#matrixmarketformat">Matrix Market Format</a>

          <h2 id="tensornetwork">Tensor Networks</h2
          >

          <p>Re: Tensor Flow</p>
<p>          This library of algorithms originated from Google's need to instruct computer systems, known as neural networks, to learn and reason similarly to how humans do, so that new applications can be derived which are able to assume roles and functions previously reserved only for capable humans; the name TensorFlow itself derives from the operations which such neural networks perform on multidimensional data arrays. These multidimensional arrays are referred to as "tensors" but this concept is not identical to the mathematical concept of tensors.[(<a href="https://en.wikipedia.org/wiki/TensorFlow">Wikipedia</a>)</p>

          <p>Re: math</p>

          <p>A tensor is the mathematical idealization of a geometric or physical quantity whose analytic description, relative to a fixed frame of reference, consists of an array of numbers1. Some well known examples of tensors in geometry are quadratic forms, and the curvature tensor. Examples of physical tensors are the energy-momentum tensor, and the polarization tensor. (<a href="http://planetmath.org/Tensor">PlanetMath</a>)</p>

          <p>A tensor is a multidimensional or N-way array. For our purposes, a tensor is a multidimensional array of complex numbers.  The rank of a tensor is the number of indices.  Thus, a rank-0 tensor is scalar (x), a rank-1 tensor is a vector (v), and a rank-2 tensor is a matrix (A ). A Tensor  Network (TN)  is  a  set  of  tensors  where  some,  or  all,  of  its  indices  are  contracted according to some pattern.  Contracting the indices of a TN is called, for simplicity,contracting the TN... Once this point is reached, it is convenient to introduce a diagrammatic notation for tensors and TNs in terms of tensor network diagrams , see Fig.(5).  In these diagrams tensors are represented by shapes, and indices in the tensors are represented by lines emerging from the shapes.  A TN is thus represented by a set of shapes interconnected by lines.  The lines connecting tensors between each other correspond to contracted indices, whereas lines that do not go from one tensor to another correspond to open indices in the TN. Using TN diagrams it is much easier to handle calculations with TN. 

          <a href="https://arxiv.org/pdf/1306.2164v3.pdf">A Practical Introduction to Tensor Networks:
Matrix Product States and Projected Entangled Pair States, Roman Orus, 2014</a></p>

<h2 id="nnmf">Non Negative Matrix Factorization</h2>
<p>Non-negative matrix factorization (NMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically. </p>

<p>Here's an example based on a text-mining application:</p>
<ul><li>
    Let the input matrix (the matrix to be factored) be V with 10000 rows and 500 columns where words are in rows and documents are in columns. That is, we have 500 documents indexed by 10000 words. It follows that a column vector v in V represents a document.</li>
    <li>Assume we ask the algorithm to find 10 features in order to generate a features matrix W with 10000 rows and 10 columns and a coefficients matrix H with 10 rows and 500 columns.</li>
    <li>The product of W and H is a matrix with 10000 rows and 500 columns, the same shape as the input matrix V and, if the factorization worked, it is a reasonable approximation to the input matrix V.</li>
    <li>From the treatment of matrix multiplication above it follows that each column in the product matrix WH is a linear combination of the 10000 row vectors in the features matrix W with coefficients supplied by the coefficients matrix H.</li></ul>

<p>This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.</p>

<p>It's useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. This follows because each row in H represents a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H.(<a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization">Wikipedia</a>)</p>

<h2 id="matrixmarketformat">Matrix Market Format</h2>
<p>The Matrix Market (MM) exchange formats provide a simple mechanism to facilitate the exchange of matrix data. In particular, the objective has been to define a minimal base ASCII file format which can be very easily explained and parsed, but can easily adapted to applications with a more rigid structure, or extended to related data objects. </p>


<p>In MM coordinate format this could be represented as follows.</p>
<p><font face="Courier New">
%%MatrixMarket matrix coordinate real general
%=================================================================================
%
% This ASCII file represents a sparse MxN matrix with L 
% nonzeros in the following Matrix Market format:
%
% +----------------------------------------------+
% |%%MatrixMarket matrix coordinate real general | <--- header line
% |%                                             | <--+
% |% comments                                    |    |-- 0 or more comment lines
% |%                                             | <--+         
% |    M  N  L                                   | <--- rows, columns, entries
% |    I1  J1  A(I1, J1)                         | <--+
% |    I2  J2  A(I2, J2)                         |    |
% |    I3  J3  A(I3, J3)                         |    |-- L lines
% |        . . .                                 |    |
% |    IL JL  A(IL, JL)                          | <--+
% +----------------------------------------------+   
%
% Indices are 1-based, i.e. A(1,1) is the first element.
%
%=================================================================================
  5  5  8
    1     1   1.000e+00
    2     2   1.050e+01
    3     3   1.500e-02
    1     4   6.000e+00
    4     2   2.505e+02
    4     4  -2.800e+02
    4     5   3.332e+01
    5     5   1.200e+01
</font>
<a href="http://math.nist.gov/MatrixMarket/formats.html">NIST</a>
          </aside>
        </script>
      </section><!-- connectionist-vs-symbolic slide ends -->

      <!-- state-of-the-art slide -->
      <section id="state-of-the-art" data-markdown>
        <script type="text/template">
          ### The State of the Art

          - Academic design for use alongside intelligent agents (AI discipline)
          - Relies on formal models or representations of knowledge &amp; language
          - Models are adapted and augmented through probabilistic methods and machine learning.
          - A small number of algorithms comprise the standard framework.
          <aside class="notes">

          </aside>
        </script>
      </section><!-- state-of-the-art slide ends -->

      <!-- traditional-nlp slide -->
      <section id="traditional-nlp" data-markdown>
        <script type="text/template">
          ### Traditional NLP Applications

          <style>
            ul.col {
              float:left;
            }
            ul.col li {
              width: 100%;
            }
          </style>

          <ul class="col">
            <li>Summarization</li>
            <li>Reference Resolution</li>
            <li>Machine Translation</li>
            <li>Language Generation</li>
            <li>Language Understanding</li>
            <li>Document Classification</li>
            <li>Author Identification</li>
            <li>Part of Speech Tagging</li>
          </ul>
          <ul class="col">
            <li>Question Answering</li>
            <li>Information Extraction</li>
            <li>Information Retrieval</li>
            <li>Speech Recognition</li>
            <li>Sense Disambiguation</li>
            <li>Topic Recognition</li>
            <li>Relationship Detection</li>
            <li>Named Entity Recognition</li>
          </ul>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- traditional-nlp slide ends -->

    </section>
    <!-- part three ends -->

    <!-- Part IV: Language Preprocessing with NLTK -->
    <section>

      <!-- part four title slide -->
      <section id="part-four" data-markdown>
				<script type="text/template">
          ## Language Preprocessing with NLTK

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part four title slide ends -->

      <!-- ingredients slide -->
      <section id="ingredients" data-markdown>
				<script type="text/template">
          ### What is Required?

          - Domain Knowledge
          - A Corpus in the Domain
          <img src="images/knowledge.png" alt="Knowledge" style="display: block; margin: 0 auto; max-height: 500px; border: none;"/>

          <aside class="notes">

          </aside>
				</script>
			</section><!-- ingredients slide ends -->

      <!-- nlp-pipeline slide -->
      <section id="nlp-pipeline" data-markdown>
				<script type="text/template">
          ### The NLP Pipeline

          <img src="images/nlp-pipeline.png" alt="The NLP Pipeline" style="max-height:600px; background:none; border:none; box-shadow:none;"/>

          <aside class="notes">

          </aside>
				</script>
			</section><!-- nlp-pipeline slide ends -->

      <!-- discourse slide -->
      <section id="discourse" data-markdown>
        <script type="text/template">
          ### Discourse

          The study of the structure of language.

          Many documents have a set structure, some document formats (like HTML)
          directly expose the structure of the communication with tags.

          ####Major tasks

          - document parsing (e.g. HTML parsing), structural decomposition
          - document clean up, targeting

          <aside class="notes">

          </aside>
        </script>
      </section><!-- discourse slide ends -->

      <!-- morphology slide -->
      <section id="morphology" data-markdown>
				<script type="text/template">
          ### Morphology

          The study of the forms of things, words in particular.

          Consider pluralization for English:

          - Orthographic Rules: puppy → puppies
          - Morphological Rules: goose → geese or fish

          Major parsing tasks:

          - stemming, lemmatization and tokenization.
          <aside class="notes">

          </aside>
				</script>
			</section><!-- morphology slide ends -->

      <!-- syntax slide -->
      <section id="syntax" data-markdown>
				<script type="text/template">
          ### Syntax
          The study of the rules for the formation of sentences.

          Major tasks: chunking, parsing, feature parsing, grammars

          <img src="images/syntax.png" alt="Syntax" style="display: block; margin: 0 auto; max-height: 500px; border: none;"/>

          <aside class="notes">
            TODO: Convert image to SVG
          </aside>
				</script>
			</section><!-- syntax slide ends -->

      <!-- semantics slide -->
      <section id="semantics" data-markdown>
				<script type="text/template">
          ### Semantics

          <img src="images/mad-hatter.png" alt="Mad Hatter" style="float:right; display: block; margin: 0 auto; width:40%; border: none;"/>

          The study of meaning
          - I see what I eat.
          - I eat what I see.
          - He poached salmon.

          Major Tasks
          - Frame extraction
          - Creation of TMRs

          <aside class="notes">
            TODO: Convert image to SVG
          </aside>
				</script>
			</section><!-- semantics slide ends -->

      <!-- tmr slide -->
      <section id="tmr" data-markdown>
				<script type="text/template">
          ### Thematic Meaning Representations

          &ldquo;The man hit the building with the baseball bat&rdquo;

          ```python
          {
              "subject": {"text": "the man", "sense": human-agent},
              "predicate": {"text": "hit", "sense": strike-physical-force},
              "object": {"text": "the building", "sense": habitable-structure},
              "instrument": {"text": "with the baseball bat" "sense": sports-equipment}
          }
          ```

          <aside class="notes">

          </aside>
				</script>
			</section><!-- tmr slide ends -->

      <!-- recent-nlp-applications slide -->
      <section id="recent-nlp-applications" data-markdown>
				<script type="text/template">
          ### Recent NLP Applications

          - [Yelp Insights](http://officialblog.yelp.com/2012/04/yelpy-insights.html)
          - Winning Jeopardy! IBM Watson
          - Computer assisted medical coding ([3M Health Information Systems](http://solutions.3m.com/wps/portal/3M/en_US/Health-Information-Systems/HIS/Products-and-Services/Computer-Assisted-Coding/))
          - Geoparsing -- [CLAVIN](http://clavin.io/) (built by Charlie Greenbacker)
          - Author Identification (classification/clustering)
          - Sentiment Analysis (RTNNs, classification)
          - Language Detection
          - Event Detection
          - [Google Knowledge Graph](http://www.google.com/insidesearch/features/search/knowledge.html)
          - Named Entity Recognition and Classification
          - Machine Translation
          - Image + Language Processing

          <aside class="notes">

          </aside>
				</script>
			</section><!-- recent-nlp-applications slide ends -->

      <!-- big-data slide -->
      <section id="big-data" data-markdown>
				<script type="text/template">
          ### Applications are BIG data

          - Examples are easier to create than rules.
          - Rules and logic miss frequency and language dynamics.
          - More data is better for machine learning, relevance is in the long tail.
          - Knowledge engineering is not scalable.
          - Computational linguistics methodologies are stochastic.

          <aside class="notes">

          </aside>
				</script>
			</section><!-- big-data slide ends -->

      <!-- tokenization-tagging slide -->
      <section id="tokenization-tagging" data-markdown>
				<script type="text/template">
          ### Tokenization &amp; Tagging

          ```python
          text = u"Medical personnel returning to New York" #...

          for sent in nltk.sent_tokenize(text):
            print(sent)
          for sent in nltk.sent_tokenize(text):
            print(list(nltk.wordpunct_tokenize(sent)))
          for sent in nltk.sent_tokenize(text):
            print(list(nltk.pos_tag(nltk.word_tokenize(sent))))
          ```

          <aside class="notes">
          <p><a href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize">sent_tokenize</a>:</p>
          <p>Return a sentence-tokenized copy of text, using NLTK’s recommended sentence tokenizer (currently PunktSentenceTokenizer for the specified language).</p>
          <p><a href="Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \w+|[^\w\s]+.">wordpunct_tokenize</a>:</p>
          <p>Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \w+|[^\w\s]+.</p>
          <p><a href="http://www.nltk.org/api/nltk.tag.html#nltk.tag.pos_tag">pos_tag</a>:</p>
          <p>Use NLTK’s currently recommended part of speech tagger to tag the given list of tokens.</p>
          </aside>
				</script>
			</section><!-- tokenization-tagging slide ends -->

			<!-- stemming-lemmatization slide -->
      <section id="stemming-lemmatization" data-markdown>
				<script type="text/template">
					### Stemming &amp; Lemmatization

					Use stemming and lemmatization to normalize words across a corpora.
            </script>
        </section>

        <!-- stemming-lemmatization slide ends -->

        <!-- stemming-1 slide -->
        <section id="stemming-1" data-markdown>
          <script type="text/template">
### Stemming

Stemming = chop off affixes to get the root stem of the word:
* running &#8594; run
* flowers &#8594; flower
* geese   &#8594; geese
</script>
					<aside class="notes">

					</aside>

			</section><!-- stemming-1 slide ends -->

			<!-- stemming-2 slide -->
      <section id="stemming-2" data-markdown>
				<script type="text/template">
        <style>
            ul.col {
              float:left;
            }
            ul.col li {
              width: 100%;
            }
          </style>
          ### Stemming

          - Stemmers
            - Lancaster (English, newer and aggressive)
              - [class nltk.stem.lancaster.LancasterStemmer](http://www.nltk.org/_modules/nltk/stem/lancaster.html#LancasterStemmer)
            - Porter (English, original stemmer)
              - [class nltk.stem.porter.PorterStemmer](http://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer)
            - Snowball (Many languages, newest)
              - [class nltk.stem.snowball.EnglishStemmer](http://www.nltk.org/_modules/nltk/stem/snowball.html#EnglishStemmer)
              - And several others for other languages

          <aside class="notes">

          </aside>
				</script>
			</section><!-- stemming-2 slide ends -->

        <!-- stemming-code slide -->
      <section id="stemming-code" data-markdown>
        <script type="text/template">
### Stemming

```python
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem.porter import PorterStemmer

text = list(nltk.word_tokenize("The women running in the fog passed bunnies working as computer scientists."))

snowball = SnowballStemmer('english')
lancaster = LancasterStemmer()
porter = PorterStemmer()
```
</script>
          <aside class="notes">

          </aside>
    
      </section><!-- stemming-code slide ends -->

       <!-- lemmatization slide -->
      <section id="lemmatization" data-markdown>
        <script type="text/template">
          ### Lemmatization

          Lemmatization = look up word form in a lexicon to get canonical lemma:
          * women   &#8594; woman
          * foxes   &#8594; fox
          * sheep   &#8594; sheep
</script>
          <aside class="notes">

          </aside>
      
      </section><!-- lemmatization slide ends -->

      <!-- lemmatization-2 slide starts -->
      <section id="lemmaization-2" data-markdown>
        ## Lemmatization
        - Lemmatizer
            - WordNet lexicon
              - [class nltk.stem.wordnet.WordNetLemmatizer](http://www.nltk.org/_modules/nltk/stem/wordnet.html#WordNetLemmatizer)

            </section>
            <!-- lemmatization-2 slide ends -->

		

			<!-- lemmatization-code slide -->
			<section id="lemmatization-code" data-markdown>
				<script type="text/template">
          ### Lemmatization

          ```python
          from nltk.stem.wordnet import WordNetLemmatizer

          lemmatizer = WordNetLemmatizer()
          lemmas = [lemmatizer.lemmatize(t) for t in text]
          print(" ".join(lemmas))
          ```

          <aside class="notes">

					</aside>
				</script>
			</section><!-- lemmatization-code slide ends -->

			<!-- stemming-lemmatization-5 slide -->
			<section id="stemming-lemmatization-5" data-markdown>
				<script type="text/template">
          ### Stemming &amp; Lemmatization
          Typical normalization of text for use as features in machine learning models looks something like this:

          ```python
          import string
          ## Module constants
          lemmatizer  = WordNetLemmatizer()
          stopwords   = set(nltk.corpus.stopwords.words('english'))
          punctuation = string.punctuation

          def normalize(text):
            for token in nltk.word_tokenize(text):
              token = token.lower()
              token = lemmatizer.lemmatize(token)
              if token not in stopwords and token not in punctuation:
                yield token

          print(list(normalize("The eagle flies at midnight.")))
          ```

          <aside class="notes">

					</aside>
				</script>
			</section><!-- stemming-lemmatization-5 slide ends -->

			<!-- parsing slide -->
			<section id="parsing" data-markdown>
				<script type="text/template">
          ### Parsing

          ```python
          import nltk

          grammar = nltk.grammar.CFG.fromstring("""
          S -> NP
          NP -> N N | ADJP NP | DET N
          ADJP -> ADJ NP
          DET -> 'an'
          N -> 'airplane'
          """)

          parser = nltk.parse.ChartParser(grammar)
          p = list(parser.parse(nltk.word_tokenize("an airplane")))

          for a in p:
            a.pprint()
            # p[0].draw()
          ```

          <aside class="notes">

					</aside>
				</script>
			</section><!-- parsing slide ends -->

			<!-- named-entity-recognition slide -->
			<section id="named-entity-recognition" data-markdown>
				<script type="text/template">
          ### Named Entity Recognition

          NLTK has an excellent MaxEnt backed Named Entity Recognizer that is trained on the Penn Treebank. You can also retrain the chunker if you would like - the code is very readable to extend it with a Gazette or otherwise.

          ```
          print(
            nltk.ne_chunk(
              nltk.pos_tag(
                nltk.word_tokenize(
                  "John Smith is from the United" 
                  "States of America and "
                  "works at Microsoft Research Labs")))
          ```
          <aside class="notes">

					</aside>
				</script>
			</section><!-- named-entity-recognition slide ends -->

      <!-- named-entity-recognition-2 begins -->
      <section id="named-entity-recognition-2" data-markdown>
        <script type='text/template'>
        ### Named Entity Recognition

        You can also wrap the Stanford NER system, which many of you are also probably used to using.

        ```
        # change the paths below to point to wherever you unzipped the Stanford NER download file
stanford_root = '/Users/benjamin/Development/stanford-ner-2014-01-04'
stanford_data = os.path.join(stanford_root, 
  'classifiers/english.all.3class.distsim.crf.ser.gz')
stanford_jar  = os.path.join(stanford_root, 
  'stanford-ner-2014-01-04.jar')

st = StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')
for i in st.tag(
  "John Bengfort is from the United States of America and "
  "works at Microsoft Research Labs".split()):
    print('[' + i[1] + '] ' + i[0])
  ```

        </script>

      </section>

    </section>
    <!-- part four ends -->

    <!-- Part V: Textual Machine Learning -->
    <section>

      <!-- part five title slide -->
      <section id="part-five" data-markdown>
				<script type="text/template">
          ## Textual Machine Learning

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part five title slide ends -->

      <!-- instances slide -->
      <section id="instances" data-markdown>
        <script type="text/template">
          Machine learning uses <em>instances</em> (examples) of data to fit a parameterized model which is used to make predictions concerning new instances.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- instances slide ends -->

      <!-- what-are-instances slide -->
      <section id="what-are-instances" data-markdown>
        <script type="text/template">
          In text analysis, what are the instances?

          <aside class="notes">

          </aside>
        </script>
      </section><!-- what-are-instances slide ends -->

      <!-- instances-are-documents slide -->
      <section id="instances-are-documents" data-markdown>
        <script type="text/template">
          Instances = Documents

          (no matter their size)

          <img src="images/book.png" alt="Book" style="width: 25%; border: none;"/>
          <img src="images/twitter.png" alt="Twitter" style="width: 25%; border: none;"/>
          <img src="images/gmail.png" alt="Gmail" style="width: 25%; border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- instances-are-documents slide ends -->

      <!-- document-collection slide -->
      <section id="document-collection" data-markdown>
        <script type="text/template">
          <img src="images/bookcollection.png" alt="Book Collection" style="border: none;"/>

          A corpus is a collection of documents to learn about.

          (labeled or unlabeled)

          <aside class="notes">

          </aside>
        </script>
      </section><!-- document-collection slide ends -->

      <!-- features slide -->
      <section id="features" data-markdown>
        <script type="text/template">
          Features describe instances in a way that machines can learn on by putting them into feature space.

          <img src="images/feature-space.png" alt="Feature Space" style="width: 25%; border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- features slide ends -->

      <!-- document-features slide -->
      <section id="document-features" data-markdown>
        <script type="text/template">
          ### Document Features

          <img src="images/document-features.png" alt="Document Features" style="float:right; border: none; width: 20%;"/>

          #### Document level features
          - Metadata: title, author
          - Paragraphs
          - Sentence construction

          #### Word level features
          - Vocabulary
          - Form (capitalization)
          - Frequency

          <aside class="notes">

          </aside>
        </script>
      </section><!-- document-features slide ends -->

      <!-- vector-encoding slide -->
      <section id="vector-encoding" data-markdown>
        <script type="text/template">
          ### Vector Encoding

          - Basic representation of documents: a vector whose length is equal to the vocabulary of the entire corpus.
          - Word positions in the vector are based on lexicographic order.

          <img src="images/vector-encoding.png" alt="Vector Encoding" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- vector-encoding slide ends -->

      <!-- token-frequency slide -->
      <section id="token-frequency" data-markdown>
        <script type="text/template">
          ### Bag of Words: Token Frequency

          - One of the simplest models: compute the frequency of words in the document and use those numbers as the vector encoding.

          <img src="images/token-frequency.png" alt="Token Frequency" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- token-frequency slide ends -->

      <!-- one-hot-encoding slide -->
      <section id="one-hot-encoding" data-markdown>
        <script type="text/template">
          ### One Hot Encoding

          - The feature vector encodes the vocabulary of the document.
          - All words are equally distant, so must reduce word forms.
          - Usually used for artificial neural network models.

          <img src="images/one-hot-encoding.png" alt="One Hot Encoding" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- one-hot-encoding slide ends -->

      <!-- tf-idf-encoding slide -->
      <section id="tf-idf-encoding" data-markdown>
        <script type="text/template">
          ### TF-IDF Encoding

          - Highlight terms that are very relevant to a document relative to the rest of the corpus by computing the term frequency times the inverse document frequency of the term.

          <img src="images/tf-idf-encoding.png" alt="TF IDF Encoding" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- tf-idf-encoding slide ends -->

      <!-- distributed-representation slide -->
      <section id="distributed-representation" data-markdown>
        <script type="text/template">
          ### Distributed Representation

          - Dense representations along a continuous scale that can be used to encode similarity within vector space.

          <img src="images/distributed-representation.png" alt="Distributed Representation" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- distributed-representation slide ends -->

      <!-- distributed-representation-example slide -->
      <section id="distributed-representation-example" data-markdown>
        <script type="text/template">
          ### Distributed Representation with gensim.models.Doc2Vec

          ```python
          >>> import gensim
          >>> documents = gensim.models.doc2vec.TaggedLineDocument("documents.txt")
          >>> model = gensim.models.doc2vec.Doc2Vec(documents, size=7, min_count=0)
          >>> model.docvecs[0]

          array([-0.0225403 , -0.0212964 ,  0.02708783,  0.0049877 ,  0.04926294,
                 -0.03268785, -0.03209411], dtype=float32)
          ```

          <aside class="notes">

          </aside>
        </script>
      </section><!-- distributed-representation-example slide ends -->

      <!-- vector-encoding-pros-and-cons slide -->
      <section id="vector-encoding-pros-and-cons" data-markdown>
        <script type="text/template">
          ### Pros and Cons of Vector Encoding

          #### Pros
          - Machine learning requires a vector anyway.
          - Can embed complex representations like TF-IDF into the vector form.
          - Drives towards token-concept mapping without rules.

          #### Cons
          - The vectors have lots of columns (high dimension)
          - Word order, grammar, and other structural features are natively lost.
          - Difficult to add knowledge to learning process.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- vector-encoding-pros-and-cons slide ends -->

      <!-- much-work slide -->
      <section id="much-work" data-markdown>
        <script type="text/template">
          In the end, much of the work for language aware applications comes from domain specific feature analysis; not just simple vectorization.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- much-work slide ends -->

     

     

     

      <!-- designing-the-problem slide-->
      <section id="designing-the-problem" data-markdown>
        <script type="text/template">
          Classification and clustering are the two fundamental techniques that are the basis of most NLP.

          It’s all about designing the problem correctly.

          <aside class="notes">

          </aside>
        </script>
      </section><!-- designing-the-problem slide ends -->

    </section>
    <!-- part five ends -->

    <!-- Part VI: Building Text Classifiers -->
    <section>

      <!-- part six title slide -->
      <section id="part-six" data-markdown>
				<script type="text/template">
          ## Building Text Classifiers

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part six title slide ends -->
 
  <!-- classification slide -->
      <section id="classification" data-markdown>
        <script type="text/template">
          ### Classification

          - Classification
            - Supervised ML
            - Requires pre-labeled corpus of documents
            - Sentiment Analysis
            - Models:
              - Naive Bayes
              - Maximum Entropy

          <aside class="notes">
<a href='#bayes'>Naive Bayes</a><br />
<a href='#maximumentropy'>Maximum Entropy</a><br />
<a href='#smoothing'>Smoothing</a><br />
<h2 id='bayes'>Naive Bayes:</h2>

<p>In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. (<a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Wikipedia</a>)</p>

<p>The probabilistic model of naive Bayes classifiers is based on Bayes’ theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption.</p>

<p>The probability model that was formulated by Thomas Bayes (1701-1761) is quite simple yet powerful; it can be written down in simple words as follows:</p>
<p>posteriorprobability=conditionalprobability⋅priorprobabilityevidence.</p>
<p> Bayes’ theorem forms the core of the whole concept of naive Bayes classification. The posterior probability, in the context of a classification problem, can be interpreted as: “What is the probability that a particular object belongs to class i given its observed feature values?” A more concrete example would be: “What is the probability that a person has diabetes given a certain value for a pre-breakfast blood glucose measurement and a certain value for a post-breakfast blood glucose measurement?”</p>

<p>One assumption that Bayes classifiers make is that the samples are i.i.d. The abbreviation i.i.d. stands for “independent and identically distributed” and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another observation (e.g., time series and network graphs are not independent).</p>

<p>An additional assumption of naive Bayes classifiers is the conditional independence of features. Under this naive assumption, the class-conditional probabilities or (likelihoods) of the samples can be directly estimated from the training data instead of evaluating all possibilities of x.</p>

<p>To illustrate this concept with an example, let’s assume that we have a collection of 500 documents where 100 documents are spam messages. Now, we want to calculate the class-conditional probability for a new message “Hello World” given that it is spam. Here, the pattern consists of two features: “hello” and “world,” and the class-conditional probability is the product of the “probability of encountering ‘hello’ given the message is spam” — the probability of encountering “world” given the message is spam.”</p>
<p>P(x=[hello,world]∣ω=spam)=P(hello∣spam)⋅P(world∣spam)</p>

(<a href="http://sebastianraschka.com/Articles/2014_naive_bayes_1.html">Naive Bayes and Text Classification
– Introduction and Theory
by Sebastian Raschka 2014</a>
)

<h2 id="maximumentropy">Maximum Entropy</h2>

<p>Intuitively, the principle is simple: model all that is known and assume nothing about that which is unknown. In other words, given a collection of facts, choose a model consistent with all the facts, but otherwise as uniform as possible. (<a href="https://aclweb.org/anthology/J/J96/J96-1002.pdf">"A Maximum Entropy Approach to Natural Language Processing " by Berger et al, 1996</a>, introductory paper)
</p>

<p>The principle of maximum entropy states that, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one with largest entropy.</p>

<p>Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. Of those, the one with maximal information entropy is the proper distribution, according to this principle.</p>

<p>In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.
 (<a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Wikipedia</a>)</p>

 <h2 name='smoothing'>Smoothing</h2>

 <p>Re: Naive Bayes</p>

 <p>If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.
 (Wikipedia)</p>

 <p>Generally:</p>

 <p>In statistics, additive smoothing, also called Laplace smoothing[1] (not to be confused with Laplacian smoothing), or Lidstone smoothing, is a technique used to smooth categorical data. Given an observation x = (x1, …, xd) from a multinomial distribution with N trials and parameter vector θ = (θ1, …, θd), a "smoothed" version of the data gives the estimator:</p>

   <p> \hat\theta_i= \frac{x_i + \alpha}{N + \alpha d} \qquad (i=1,\ldots,d),</p>

<p>where the pseudocount α > 0 is the smoothing parameter (α = 0 corresponds to no smoothing). Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical estimate xi / N, and the uniform probability 1/d. Using Laplace's rule of succession, some authors have argued[citation needed]that α should be 1 (in which case the term add-one smoothing[2][3] is also used), though in practice a smaller value is typically chosen.</p>

<p>From a Bayesian point of view, this corresponds to the expected value of the posterior distribution, using a symmetric Dirichlet distribution with parameter α as a prior. In the special case where the number of categories is 2, this is equivalent to using a Beta distribution as the conjugate prior for the parameters of Binomial distribution.
(<a href="https://en.wikipedia.org/wiki/Additive_smoothing">Wikipedia</a>)</p>
          </aside>
        </script>
      </section><!-- classification slide ends -->


 <!-- classification-pipeline slide-->
      <section id="classification-pipeline" data-markdown>
        <script type="text/template">
          ### Classification Pipeline

          <img src="images/classification-pipeline.png" alt="Classification Pipeline" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- classification-pipeline slide ends -->
    </section>
    <!-- part six ends -->

    <!-- Part VII: Building Topic Models -->
    <section>

      <!-- part seven title slide -->
      <section id="part-seven" data-markdown>
				<script type="text/template">
          ## Building Topic Models

          <aside class="notes">

          </aside>
				</script>
			</section><!-- part seven title slide ends -->


      <!-- clustering slide -->
      <section id="clustering" data-markdown>
        <script type="text/template">
          ### Clustering

          - Clustering
            - Unsupervised ML
            - Groups similar documents together
            - Topic Modeling
            - Models:
              - K means
              - LDA

          <aside class="notes">
<a href="#lda">LDA - Latent Dirichlet allocation</a>
<a href="#lsi">LSI - Latent Semantic Indexing</a>

<h2 id="lda">LDA</h2>
<p>In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.[1] Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.[2]</p>

<p>In LDA, each document may be viewed as a mixture of various topics. This is similar to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a Dirichlet prior. In practice, this results in more reasonable mixtures of topics in a document. It has been noted, however, that the pLSA model is equivalent to the LDA model under a uniform Dirichlet prior distribution.</p>
<p>For example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as milk, meow, and kitten, which can be classified and interpreted by the viewer as "CAT_related". Naturally, the word cat itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: puppy, bark, and bone might have high probability. Words without special relevance, such as the (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of supervised labeling and (manual) pruning on the basis of their likelihood of co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.</p>

<p>Each document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.</p>
(<a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Wikipedia</a>)

<p>In more detail, LDA represents documents as mixtures of topics that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you</p>

   <ul><li> Decide on the number of words N the document will have (say, according to a Poisson distribution).</li>
    <li>Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals.</li>
    <li>Generate each word w_i in the document by:
        First picking a topic (according to the multinomial distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).</li>
        <li>Using the topic to generate the word itself (according to the topic’s multinomial distribution). For example, if we selected the food topic, we might generate the word “broccoli” with 30% probability, “bananas” with 15% probability, and so on.</li></ul>

<p>Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.</p> (<a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">Blog post by Edwin Chen whoever he is, but I like his style</a>)

<h2 id="lsi">LSI = Latent Semantic Indexing</h2>

<p>Latent semantic indexing adds an important step to the document indexing process. In addition to recording which keywords a document contains, the method examines the document collection as a whole, to see which other documents contain some of those same words. LSI considers documents that have many words in common to be semantically close, and ones with few words in common to be semantically distant. This simple method correlates surprisingly well with how a human being, looking at content, might classify a document collection. Although the LSI algorithm doesn't understand anything about what the words mean, the patterns it notices can make it seem astonishingly intelligent. </p> (<a href="http://c2.com/cgi/wiki?LatentSemanticIndexing">Wild internet place</a>

<a href="http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf">Summary of Grossman and Frieder’s 
Information Retrieval, Algorithms and Heuristics, with an example of LSI which I suggest reading the whole thing 
</a>
          </aside>
        </script>
      </section><!-- clustering slide ends -->

       <!-- topic-modeling-pipeline slide-->
      <section id="topic-modeling-pipeline" data-markdown>
        <script type="text/template">
          ### Topic Modeling Pipeline

          <img src="images/classification-pipeline.png" alt="Classification Pipeline" style="border: none;"/>

          <aside class="notes">

          </aside>
        </script>
      </section><!-- topic-modeling-pipeline slide ends -->

    </section>
    <!-- part seven ends -->

    <!-- Concluding Remarks -->
    <section>

      <!-- conclusion slide -->
      <section id="conclusion" data-markdown>
				<script type="text/template">
          ## Concluding Remarks

          <aside class="notes">

          </aside>
				</script>
			</section><!-- conclusion slide ends -->

      <!-- sprints slide -->
      <section id="sprints" data-markdown>
				<script type="text/template">
          ## Sprints

          - Baleen: RSS/Atom ingestion tool
          - Trinket: Visual model analyzer
          - [See the PyCon Sprints schedule for more details](https://us.pycon.org/2016/community/sprints/)
          </script>
          <aside class="notes">

          </aside>
	
			</section><!-- sprints slide ends -->

      <!-- questions slide -->
      <section id="questions" data-background="images/question_mark.jpg" data-background-size="800px" data-markdown>
				<script type="text/template">
          <h3><font color="white">Questions?</font></h3>
          
          <small><a href="http://bit.ly/25qgRN8">http://bit.ly/25qgRN8</a></small>
          </script>

          <aside class="notes">

          </aside>
			</section><!-- questions slide ends -->

    </section>
    <!-- concluding remarks ends -->


		</div><!-- slides ends -->
	</div><!-- reveal ends -->

	<script src="lib/js/head.min.js"></script>
	<script src="js/reveal.js"></script>

	<script>
		// More info https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			history: true,

			// More info https://github.com/hakimel/reveal.js#dependencies
			dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
			]
		});
	</script>
</body>
</html>
